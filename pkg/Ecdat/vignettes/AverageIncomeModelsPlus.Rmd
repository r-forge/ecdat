---
title: "Economic impact of presidents and war: Did FDR or WW2 End the Great Depression?"
author: "Spencer Graves and Jouni Helske"
date: "`r Sys.Date()`"
output:
    rmarkdown::html_vignette:
        fig_caption: yes
        fig_width: 6
        fig_height: 5 
vignette: >
  %\VignetteIndexEntry{Average Income Models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}{inputenc}
---
```{r setup, include = FALSE, cache = FALSE}
library(RefManageR)
bibFile <- system.file("Bib", "biblatexExamples.bib", 
                           package = "RefManageR")
bib <- ReadBib(bibFile, check = FALSE)
bibF2 <- "AvgInc.bib"
bib2 <- ReadBib(bibF2) 
BibOptions(check.entries = FALSE, style = "markdown", cite.style = "authoryear",
           bib.style = "numeric")
```

# Abstract 

Economists claim that World War II (WW2) not Franklin Roosevelt (FDR) ended the Great Depression. U.S. average annual income (Gross Domestic Product, GDP, per capita adjusted for inflation) grew by 10.6 percent per year during WW2 (1939-1945) and only 6.2 percent per year under FDR before the war (1933-1939). If WW2 not FDR ended the Great Depression, we would expect to see (a) similar growth spurts during other wars, and (b) sufficient variability between the economic growth during other presidencies to make FDR's performance consistent with that of other presidents.  We analyze GDP data 1790-2015 with unemployment 1800-2015.  Our findings have implications for both economics and statistical modeling:  (1) We find a president but not a war effect on GDP.  (2) We find a war but not a president effect on unemployment.  (3) The correlation between the state-space increments of the levels of GDP and unemployment was (-1) consistent with Okun's Law.  (4) Nonlinear optimization is still difficult.  Constrained optimizers seemed to give the best results for models where we saw estimation difficulties;  in those cases, we preferred parameterizations in terms of standard deviations and correlations.  

**Key Words**: Kalman filtering and smoothing, KFAS, R (programming language), econometrics, gross domestic product (GDP), unemployment, Okun's Law  

# Introduction 

We begin this article with a brief review of typical perspectives of US economic performance during the Great Depression and World War II.  We then consider a few plots of available data.  That is followed by a summary of models we fit.  This is an interim report on research discussed in a vignette on "Modeling Gross Domestic Product and unemployment with the KFAS package for R" included with the [Ecdat](https://rweb.crmda.ku.edu/cran/web/packages/Ecdat/Ecdat.pdf) package for R, with the most current version available from [R-Forge](https://r-forge.r-project.org/R/?group_id=1439).  

In brief, we do not find growth spurts associated with wars other than WW II, but we do find that major wars have driven down unemployment.  Moreover, the economic growth of the US economy under FDR dramatiacally exceeded that of other presidents. Beyond that we found a correlation of (-1) between state-space increments of the levels of GDP and unemployment, consistent with [Okun's Law](https://en.wikipedia.org/wiki/Okun%27s_law).

For this we use the [KFAS (Kalman filtering and smoothing)](https://rweb.crmda.ku.edu/cran/web/packages/KFAS/KFAS.pdf) package for R.  This work exposed problems using the standard [optim](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/ts.html) nonlinear optimizer for some of our models.  We initially followed the example of the [nlme](https://rweb.crmda.ku.edu/cran/web/packages/nlme/nlme.pdf) package in parameterizing variance parameters on a log scale.  When we perceived problems with that, we tried constrained optimization, parameterizing the problem in terms of standard deviations and correlations, as suggested by `r Citet(bib2, "lme4vignette")`.  In one such case, a standard optim(..., method='L-BFGS-B') call stopped with a negative standard deviation.  That problem was overcome by switching to a different nonlinear optimizer, as suggested by `r Citet(bib2, "lme4vignette")`.  However, our limited tests suggest that the other optimizers may be more fragile than optim(..., method='L-BFGS-B'), and the best strategy may be to parameterize everything on constrained square root and correlation scales, and try at least two different optimizers.  

For average annual income (GDP per capita, adjusted for inflation) we use data from `r Citet(bib2, "MeasuringWorth")`.  For unemployment we used multiple sources, documented with the [USGDPpresidents](https://rweb.crmda.ku.edu/cran/web/packages/Ecdat/USGDPpresidents.pdf) data.frame in the [Ecdat](https://rweb.crmda.ku.edu/cran/web/packages/Ecdat/Ecdat.pdf) package for R.  

# US Economic Performance during the Great Depression and World War II  

The Legacy of the [Great Depression](https://en.wikipedia.org/wiki/Great_Depression) and the [New Deal](https://en.wikipedia.org/wiki/New_Deal) still carry profound implications for the current economic policies of the world's most economically advanced countries.  `r Citet(bib2, "Horowitz2011")` recently claimed that Hoover had more aggressive deficit spending and public works programs than [Franklin Roosevelt (FDR)](https://en.wikipedia.org/wiki/Franklin_D._Roosevelt), and far from contributing to the recovery these programs actually made the economy worse.  He concluded that "The persistence of the Hoover myth continues to justify the counterproductive policies of the Obama administration and thereby prevents markets from generating the economic recovery of which they are fully capable." However, Horowitz's report discusses no model fits, includes no tables, and plots only "real federal spending" from 1929 to 1934.

The dominant view of the evolution of the US economy between 1933 and 1945 is that World War [World War II (WW II)](https://en.wikipedia.org/wiki/Great_Depression#World_War_II_and_recovery), not FDR, ended the [Great Depression](https://en.wikipedia.org/wiki/Great_Depression).  This perspective is supported by our review of the available unemployment data but superficially contradicted by our naive analyses of average annual income in the US ([real US Gross Domestic Product (GDP) per capita](https://en.wikipedia.org/wiki/Real_gross_domestic_product), 1790-2015, from [MeasuringWorth](http://measuringworth.com/)): First, other wars have not been accompanied by such obvious growth spurts. Second, FDR's economic performance even without World War II seems unmatched by any other U.S. president.  

`r Citet(bib2, "McChesney2014")` said, "In earlier eras it had been assumed that there was an economic 'guns and butter' tradeoff, and that military spending had to occur at the expense of other sectors of the economy.  However, one of the lessons of the economic expansion in Nazi Germany, followed by the experience of the United States itself in arming for the Second World War, was that big increases in military spending could act as huge stimulants to the economy." 

Other research identified alternatives that would reportedly produce more jobs than military spending:  Tax cuts for household consumption and increased spending on clean energy, health care and education would produce 27, 47, 69 and 151 percent more jobs, respectively, than military spending, according to `r Citet(bib2, "Pollin2009")`.  

However, these claims are not universally accepted among economists.  `r Citet(bib2, "Barro2013")` cited a dozen different empirical studies that estimated the multiplier effect of military spending.  These studies provide estimates that range from less than zero to roughly 1.4 with a median of 0.6 for those with numbers in Barro's table.  Barro explained that if this multiplier is greater than 1, it means that cuts in defense spending also reduce the private portions of GDP.  If it's positive but less than 1, it means that the private sector increases but by less than the fall in defense.  If it's negative it means that private economic activity rises by more than the fall in defense spending. The work of `r Citet(bib2, "Pollin2009")` suggests that the multiplier effect of a cut in military spending depends on what is done with the defense savings.  

Beyond this, the work of [Milton Friedman](https://en.wikipedia.org/wiki/Milton_Friedman) and others suggests that models like these should also consider [inflation](https://en.wikipedia.org/wiki/Inflation) and [unemployment](https://en.wikipedia.org/wiki/Unemployment);  we consider unemployment but not inflation.    

Whatever the specific cause, the growth of the U.S. economy under [FDR](https://en.wikipedia.org/wiki/Franklin_D._Roosevelt) before the war signficantly exceeds that of any other president (not counting the negative record of [Herbert Hoover](https://en.wikipedia.org/wiki/Herbert_Hoover)).

This document is a summary of research not yet completed but described in detail in a vignette on "Modeling Gross Domestic Product and unemployment with the KFAS package for R" included with the [Ecdat](https://rweb.crmda.ku.edu/cran/web/packages/Ecdat/Ecdat.pdf) package, with the most current version available from [R-Forge](https://r-forge.r-project.org/R/?group_id=1439).    

We used GDP data from `r Citet(bib2, "MeasuringWorth")`, included as the [USGDPpresidents](http://rpackages.ianhowson.com/cran/Ecdat/man/USGDPpresidents.html) data set in the Ecdat package (currently only available in the [R-Forge](https://r-forge.r-project.org/R/?group_id=1439) version of the package).  This [data.frame](https://stat.ethz.ch/R-manual/R-devel/library/base/html/data.frame.html) also includes unemployment data from multiple sources as documented int its help page.  

We have conclusions for both economics and statistical methods:  

    (E1) The models we fit failed to find a signficant effect of war on real GDP per capita but did find that the performance of the U.S. economy under the Hoover and FDR administrations was dramatically different from that for any other presidency. 
  
    (E2) We reach the opposite conclusion from modeling unemployment:  We find a war but not an administration effect.
  
    (E3) We present a state-space version of Okun's law, estimating a correlation of (-1) in the random increments for the state space components representing the levels of unemployment and GDP.  
  
    (E4) These conclusions must be considered tentative, because these models do not consider other variables like inflation that might impact economic growth and unemployment, and our work is still in progress.  
  
    (S1) The companion vignette included with the Ecdat package for R includes several examples of convergence problems using the standard optim numerical optimizer. Most of these came from using unconstrained optimization over log(variance) parameters, following the pattern in the nlme package for R.  When we lacked confidence in the unconstrained optimization, we reparameterized in terms of standard deviations and correlations;  Bates et al. (2014) say that a quadratic will generally approximate the log(likelihood) surface better in that parameterization.  Sadly, we encountered problems also with constrained optimization.  In one case optim(..., method = 'L-BFGS-B') failed while testing parameter values out of bounds.  We fixed this by using an alternative optimizer.  Unfortunately, we have so far not found a single constrained optimizer that seemed to work well for us in all cases.  We're inclined to start with optim(..., method = 'L-BFGS-B') and switch to BOBYQA in the minqa package or Nelder-Mead, BOBYQA, or COBYLA in the nloptr package if we encounter a problem or want to check convergence.  However, this is not based on exhaustive testing.  
    
    (S2) More generally, KFAS provides a powerful set of tools for modeling extensions to the present analysis, e.g., mixing annual and quarterly data or adding other variables such as inflation to our models of GDP per capita with unemployment.

# Initial Plots

We start by plotting the [MeasuringWorth](http://measuringworth.com/) estimates of average annual income in the US (adjusted for inflation) from 1790 to 2015:

```{r, echo=FALSE, fig.cap = "real US GDP per capita (thousands of 2009 $)"}
suppressMessages(library(Ecdat))
GDPna <- is.na(USGDPpresidents$realGDPperCapita)
GDP. <- USGDPpresidents[!GDPna,]
plot(realGDPperCapita/1000~Year, GDP., log='y',
  type='l', ylab='thousands of 2009 dollars', 
  las=1, main='Real US GDP per capita')     
```     

This plot suggests relatively steady growth from 1790 to 2015 with the exception of a dramatic rise roughly two thirds of the way through the series preceded and followed by declines.  

Many readers will likely suspect that this exceptional period corresponds to the [Great Depression](https://en.wikipedia.org/wiki/Great_Depression), [World War II (WW II)](https://en.wikipedia.org/wiki/Great_Depression#World_War_II_and_recovery), and the administrations of [Herbert Hoover](https://en.wikipedia.org/wiki/Herbert_Hoover) and [Franklin Roosevelt (FDR)](https://en.wikipedia.org/wiki/Franklin_D._Roosevelt).  

We now ask the reader:  Can you see in this plot any macroeconomic impact of the First World War, the Civil War or any other war in US history?  

It may be there, but it does not jump at us like the Hoover-FDR period does. To help us evaluate this, we created a variable for war, defined as a violent conflict that averaged over 10 battle deaths per million population per year.  To operationalize that definition, we created variables for approximate battle deaths per year (battleDeaths) and approximate battle deaths per million population per year (battleDeathsPMP).  These varables are included in [USGDPpresidents](http://rpackages.ianhowson.com/cran/Ecdat/man/USGDPpresidents.html).  We plot battleDeathsPMP here with labels for the wars and for the Hoover-FDR period:  

```{r, echo=FALSE, fig.cap = "Battle deaths per million population"}
GDP.$executive <- ordered(GDP.$executive)
GDP.$war <- ordered(GDP.$war)

plotPresWars <- function(x, Data, ..., 
     yHoover=0.8, yFDR=2, yWar=seq(1.7, by=-.1, len=9), 
     cex.war=0.67, lwd.war=1.15, col.war='red'){
  X <- eval(substitute(x), Data)
  plot(X, ...)
  abline(v=c(1929, 1933, 1945), lty='dashed')
  usr <- par('usr')
  Hy <- (usr[3]+yHoover*diff(usr[3:4]))
  if(par('ylog'))Hy <- exp(Hy)
  text(1930, Hy, 'Hoover', srt=90, cex=.9, xpd=NA)
  Ry <- (usr[3]+yFDR*diff(usr[3:4]))
  if(par('ylog'))Ry <- exp(Ry)
  text(1939, Ry, 'FDR', srt=90, cex=1.1, 
       col='blue', xpd=NA)
  yrMid <- mean(usr[1:2])
  dyr <- diff(usr[1:2])
  y. <- seq(usr[3], usr[4], len=5)
  dy <- diff(y.[c(2,4)])
  y2.5 <- mean(y.[c(2,3)])
  wars <- levels(Data$war)
  nWars <- length(wars)
  for(i in 2:nWars){
    w <- wars[i]
    sel <- (Data$war==w)
    yrs <- range(Data$Year[sel])
    abline(v=yrs, lty='dotted', 
           col=col.war, lwd=lwd.war)
    yr. <- mean(yrs)
    wy <- (y2.5 + yWar[i-1]*dy)
    if(par('ylog'))wy <- exp(wy)
    text(yr., wy, w, srt=90, col='red', 
         cex=cex.war, xpd=NA)
  }
  invisible("done")
}

plotPresWars(battleDeathsPMP~Year, GDP., 
    yHoover=0.2, yFDR=.8, 
    yWar=seq(.1, .2, len=9), type='l',
     ylab='battle deaths per million pop', las=1, 
     main='Battle deaths during war')     
```     

The 2001-2014 [War in Afghanistan](https://en.wikipedia.org/wiki/War_in_Afghanistan_(2001%E2%80%9314)) and the 2003-2011 [Iraq war](https://en.wikipedia.org/wiki/Iraq_War) do not appear here, because together they averaged fewer than 3 battle deaths per year per million population.  One might argue that they were more like the [Super Bowl](https://en.wikipedia.org/wiki/Super_Bowl) than World War II in terms of their impact on the U.S. economy.  

We also model unemployement, with numbers taken from four different sources as documented in [help(USGDPpresidents)](http://rpackages.ianhowson.com/cran/Ecdat/man/USGDPpresidents.html):  

```{r, echo=FALSE, fig.cap = "Presidents, wars & unemployment"}
GDP <- GDP.
GDP$realGDPperCapita <- ts(GDP$realGDPperCapita, 
                             GDP$Year[1])
GDP$unemployment <- ts(GDP$unemployment, 
                             GDP$Year[1])
plotPresWars(unemployment, GDP, 
             main=paste0('US unemployment'), 
             ylab='unemployment rate', las=1, 
             xlab='', yHoover=0.07, yFDR=0.25, 
             yWar=seq(.5, by=-.05, len=9)) 
```

The unemployment numbers prior to 1890 clearly do not track the state of the economy as well as the more recent data.  More to the point of the present discussion, the unemployment climbed almost vertically during the Hoover years and fell dramatically during the administration of FDR.  Closer study suggests that unemployment fell rapidly during all wars for which we have reasonable data except for Vietnam.  It's less clear from this image if unemployment fell more rapidly during FDR but prior to World War II than it has under other presidencies.  We need models for that -- and, as noted elsewhere in this report, the models did not support an exceptional FDR effect on unemployment.

# Choice of scales 

We model income data on a log scale, consistent with the above plot of realGDPperCapita.  

We model unemployment on a probit scale for multiple reasons:  First, simple logic says that nothing can be truely normally disributed if the numbers must be bounded, e.g., all positive like variances or between 0 and 1 like correlations.  Second, the [Shapiro-Wilks](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html) test for normality of first differences of the raw unemployment numbers since 1890 vs. log, logit and probit transformations gave significance probabilities of 5e-5, 4e-5, 6e-5 and 1e-4:  The increments on none of these scales are independent normal with constant variance.  However, the numbers come from different sources, and our best models for unemployment included estimated 3 levels of observation error variance, with the older data being less reliable, as we might expect.   Finally, the probit has the additional interpretation of an accumulation of many small additive adjustments, which should make the result normal in most cases by the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem).  

To look at the difference, we redo here the above plot of unemployment:  

```{r, echo=FALSE, fig.cap = "Presidents, wars & probit(unemployment)"}
plotPresWars(qnorm(unemployment), GDP, 
    main=paste0('US unemployment'), 
    ylab='unemployment rate', las=1, xlab='',
    yHoover=0.07, yFDR=0.25, 
    yWar=seq(.5, by=-.05, len=9), axes=FALSE) 
axis(1)
unempScale <- c(.02, .05, .1, .2)
axis(2, qnorm(unempScale), unempScale)
```

The dramatic nature of the impact of World War II is more visible in this plot than in the above plot of the untransformed data.  We think most serious observers would agree that it's more difficult to reduce unemployment from 2 percent to 1.2 percent (the lowest level recorded) than from 10 percent to 9 percent, for example.  Use of this transformed scale provides an automatic acknowledgement of that increased difficulty.  
[USGDPpresidents](https://rweb.crmda.ku.edu/cran/web/packages/Ecdat/USGDPpresidents.pdf) includes a variable, "unempSource" indicating the source of the numbers used here.  This is an ordered factor, and we can order a count of observations by its levels:  

```{r echo=FALSE, unempSource}
(unempS <- table(GDP$unempSource))
```

The help page for USGDPpresidents says that Lebergott is the source for the data until 1890, and Romer's data are used through 1930.  The current standard source, [the US Bureau of Labor Statistics (BLS)](https://en.wikipedia.org/wiki/Bureau_of_Labor_Statistics), has data starting in 1940.  

Economists also claim that low unemployment tends increase [inflation](https://en.wikipedia.org/wiki/Inflation), a relationship called the [Phillips curve](https://en.wikipedia.org/wiki/Phillips_curve).  [Another reason for using a transformation like probits, i.e., qnorm(unemployment), is that it might tend to linearize the "curve" that `r Citet(bib2, "Philips")` observed in unemployment in the [United Kingdom](https://en.wikipedia.org/wiki/United_Kingdom) from 1913 to 1948.)  

Inflation is widely calculated from changes in a price index, usually the consumer price index.  The CPI is included in the GDP data.frame, but we'll skip that for the present discussion.  

We next briefly outline the general Kalman framework.  This is followed by an overview of the alternative models fit so far in the companion vignette included in the [Ecdat](https://rweb.crmda.ku.edu/cran/web/packages/Ecdat/Ecdat.pdf) package for R.  This article ends with a brief discussion of future work in this area.  

# Kalman Filtering and Smoothing 

The theory behind these models can be described in terms of a 3-step Bayesian updating cycle to add new observations to a probability distribution summarizing the available knowledge about the state of the system under study `r AutoCite(bib2, "Graves07")`:

  1.  Receive and store new data.  
  
  1.  Create a prior distribution for the new data by modifying the posterior computed after the last observation to allow for possible changes in state in the time between the last and current observations.    
  
  1.  Combine the new data with the prior to produce a posterior distribution of the state.  
  
Three different names are commonly used for this class of models.  The oldest name is 'Kalman filtering (and smooting)', recognizing the path-breaking contributions of [Rudolf E. Kálmán](https://en.wikipedia.org/wiki/Rudolf_E._K%C3%A1lm%C3%A1n).  A more descriptive term for this class of models is "state space", recognizing the central role of the indirectly observed "state" in these models;  this term is used by `r Citet(bib2, "DurbinKoopman")`.  `r Citet(bib2, "West97")` call these "Dynamic (linear) models."

In the present discussion, the system under study is the US economy at various times, which we will summarize in a vector of one or more numbers that represent the state or condition of the economy at each point in time.  (The term "state" in "state space" refers to this type of representation.)  The data will be annual observations on real US GDP per capita (realGDPperCapita) and / or unemployment, as previously discussed.    

Our primary reference for Kalman filtering is `r Citet(bib2, "DurbinKoopman2")`.  In brief, we assume that time series observations $y_t$, $t$ = 1, 2, ..., are linear functions of some latent state vector,  $\alpha_t$, plus observation error:  

$$y_t = Z_t \alpha_t + \epsilon_t$$

$$\alpha_{t+1} = T_t \alpha_t + R_t \eta_t.$$

where $\epsilon_t \tilde\ N(0, H_t)$, $\eta_t \tilde\ N(0, Q_t)$ and $\alpha_1 \tilde\ N(a_1, P_1)$, independent of each other.  In this, $y_t$ is a $p$-vector (where $p$ may actually vary with $t$), $\alpha_t$ is $m$-dimensional, and $\eta_t$ has dimension $k$.  (See `r Citet(bib2, "DurbinKoopman")` for more detail on the theory behind this model or `r Citet(bib2, "KFASarticle")` for more on the KFAS implementation.)   

The first of these two equations is called the "observtion equation".  The second is called the "state transition equation".  When a new observation arrives, we first degrade the posterior from the previous observation to obtain a prior for the new observation to model the loss of knowledge about the state by the addition of migration noise per the state transition equation.  Then we apply Bayes' theorem to get a new observation, as outlined with the 3-step cycle above.  This is Kalman filtering.  Kalman smoothing then runs time backward using similar math to produce posterior estimates of the state at each point in time given all the data, not just the previous data.  

In the models considered here, $y_t$ is log(realGDPperCapita) or qnorm(unemployment) or both. The dimensionality of $\alpha_t$ will vary, but the first component(s) will always be the level.  With a univariate response, $Z_t$ will be a row vector with 1 in the first position and 0 elsewhere. With a bivariate respose, $Z_t$ will have two rows, with the first two columns being an identity matrix and the rest being zero.  

This will force $Q_t$, $T_t$, and $P_1$ to change between models.  In addition, some models have diffuse initialization for the state vector.  These are specified by a model component $P_{1,\infty}$;  see `r Citet(bib2, "KFASarticle")` or [SSmodel](http://rpackages.ianhowson.com/cran/KFAS/man/logLik.SSModel.html).  

And the different models require estimating different parameters.  These pieces are summarized in the following table:  

  || Response | Model | state vector compo- nents | time-varying compo-nents | P1 | P1inf | parameters to estimate 
-:|:-----|------------------|:-----|:----:|:---:|:---:|:--------------
1 | GDP/cap | EWMA1: Standard Bayesian EWMA | 1:level | | 0 | 1 | 2:Q, H
2 | GDP/cap | EWMA2: double EWMA | 2:level, slope | | 0 | diag(2) | 3:Q.lvl, Q.slope, H
 | | | | | | | |  
3 | GDP/cap | EWMA2pres1: EWMA2 allowing slope to change only between presidents | 2:level, slope | Q | 0 | diag(2) | 3:Q.lvl, Q.slope, H 
4 | GDP/cap | EWMA2pres2: EWMA2 estimating mean slope and no continuity between presidents | 3:level, slope, a1.slope | T, Q | diag(c(0, NA, 0)) | diag(c(1, 0, 0)) | 4:a1.slope, Q.lvl, Q.slope, H
5 | GDP/cap | EWMA2pres3: EWMA2 with an adjustment for each president | 3:level, slope, pres3 | T, Q | diag(c(0, 0, NA) | diag(c(1, 1, 0)) | 4:Q.lvl, Q.slope, Q.pres3, H 
 | | | | | | | |  
6 | GDP/cap | EWMA2war0: EWMA2 with fixed effect of war | 4:level, slope, war, deaths | T | 0 | diag(4) | 3:Q.lvl, Q.slope, H
7 | GDP/cap | EWMA2war1: EWMA2 with random effect of war | 4:level, slope, war, deaths | T, Q | diag(c(0, 0, Q.war))	| diag(c(1, 1, 0, 0)) | 6:Q.lvl, Q.slope, Q.war, Q.deaths, Q.warDeaths, H
8 | GDP/cap | EWMA2war2: EWMA2 with variance effect for war |	2:level, slope |	Q |	0 | diag(2) | 5:Q.lvl, Q.slope, Q.war, Q.deaths, H
 | | | | | | | |  
9 | unemp | uEWMA1: Standard Bayesian EWMA | 1:level | | 0 | 1 | 2:Q, H
10 | unemp | uEWMA1a: EWMA with higher error variance for older data | 1:level | H | 0 | 1 | 5:Q, H1..4 
 | | | | | | | | | 
11 | unemp | uEWMA1pres3: EWMA with a slope adjustment for each president | 2:level, pres3 | T, Q, H | diag(c(0, NA)) | diag(c(1, 0)) | 6:Q.lvl, Q.pres3, H1..4
12 | unemp | uEWMA1war0: EWMA with fixed slope adjustments for war | 3:level, war, deaths | T, H | 0                 | diag(3)          | 5:Q.lvl, H1..4 
13 | unemp | uEWMA1war1: EWMA with radom slope adjustments for war            | 3:level, war, deaths        | T, Q, H | diag(c(0, Q.war))      | diag(3)             | 8:Q.lvl, Q.war, Q.deaths, Q.warDeaths, H1..4 
 | | | | | | | | | 
14 | GDP/cap & unemp | Okun: [Okun's law](https://en.wikipedia.org/wiki/Okun's_law) | 3:GDP.lvl, unemp.lvl, GDP.slope | H | 0 | diag(3) | 10:Q.pl, Q.ul, Q.ps, Q.pl.ul, Q.ul.ps, Hp, Hu1..4 

We will limit the discussion in this interim report to these models.  We expect, however, that fitting other similar models would likely support a deeper understanding of the macroeconomic impact of presidents and wars.   

The following table summarizes the fit obtained from these models starting with the number of degrees of freedom with the estimated marginal likelihood and the root mean square prediction errors for each model.  The number of degrees of freedom is the number of parameters estimated by nonlinear iteration plus the number of diffuse states;  the latter is obtained as sum($P_{1,\infty}$), since $P_{1,\infty}$ is a 0-1 matrix with 1's on the diagonal for each state with a diffuse prior.  The table also includes columns summarizing whether the nonlinear estimation seemed to converge when parameterized in terms of log(variances) [logScale = y or n] and whether a constrained optimization was attempted and if so, whether it returned an error code or returned the best fit ("failed" vs. y or n;  "-" if not attempted):  




** Table of ndf + marginal likelihood (but not diffuse likelihood) + rmse.lnGDPperCap + rmse.probitUnemp + logScale + constr + compare + p-values 

Need to cite paper on diffuse & marginal likelihood 
+ 2*log(LR) ~ approx chi-square in Wikipedia 
+ Pinheiro and Bates 
+ Doug Bates et al. re concerns ... .




# President effect on GDP 



* Discuss war effect on GDP 


# Unemployment and war 




# Okun's law 




# Future work 


** JOUNI **: What about the differences in the marginal likelihood between the univariate and bivariate formulations of the Okun's law model?  



```{r, echo=FALSE}
suppressMessages(library(KFAS))
EWMA1fmla <- log(realGDPperCapita)~
  SSMtrend(1,Q=list(matrix(NA)), a1=log(realGDPperCapita[1]),
           ynames='lnGDPperCap')
#We use this to create an SSModel as follows:  
EWMA1mdl <-SSModel(EWMA1fmla, GDP, H=matrix(NA) )
# This model has 2 unknowns, 
#     the NA's in EWMA1fmla and SSModel: Q & H  

EWMA1fit <- fitSSM(EWMA1mdl, inits=c(lnQ=0, lnH=0))

EWMA1pred <- predict(EWMA1fit$model, 
    interval='prediction', filtered=TRUE)

EWMA1fitM <- fitSSM(EWMA1mdl, 
    inits=EWMA1fit$optim.out$par, marginal=TRUE)
#EWMA1fit$optim.out$par
#EWMA1fitM$optim.out$par
#logLik(EWMA1fit$model)
#logLik(EWMA1fitM$model, marginal=TRUE)

EWMA1.KFS <- KFS(EWMA1fit$model)
nInf <- length(EWMA1.KFS$Finf)
N <- nrow(EWMA1.KFS$model$y)
i. <- (nInf+1):N
#-0.5*with(EWMA1.KFS, sum(log(Finf)) + #(N-nInf)*log(2*pi) +
#            sum(log(F[i.]) + v[i.]^2/F[i.]) )
#sqrt(mean(EWMA1.KFS$v^2))

#Let's wrap these three goodness-of-fit measures 
#   in a fuction ... 
logLik2 <- function(object, ...){
  ll2 <- data.frame(ndf=NA, diffuse=NA, 
      marginal=NA, rmse.lnUSD=NA, 
      rmse.pbtUnemp=NA, 
      stringsAsFactors=FALSE)
  ll2[1] <- (length(object$optim.out$par)
             + sum(object$model$P1inf) )
  ll2[2] <- logLik(object$model)
  ll2[3] <- logLik(object$model, marginal=TRUE)
  K. <- KFS(object$model)
  Ks <- apply(K.$v^2, 2, function(x)
        sqrt(mean(x, na.rm=TRUE)))
  if(length(Ks)<2){
    Ynames <- colnames(object$model$y)
    if((!is.null(Ynames)) && (Ynames=='probitUnemp')){
      ll2[5] <- Ks
    } else ll2[4] <- Ks
  } else ll2[4:5] <- Ks
#
  ll2
}
#data.frame to store these numbers 
#   for the different models we'll fit:  

mdls <- c('EWMA1', 'EWMA2', 'EWMA2pres1', 
   'EWMA2pres2', 'EWMA2pres3', 'EWMA2war0',
   'EWMA2war1', 'EWMA2war2', 'uEWMA1', 'uEWMA1a', 
   'uEWMA1pres3', 'uEWMA1war0', 'uEWMA1war1',
   'Okun')
n.mdls <- length(mdls)
logLiks <- data.frame(model=mdls, 
    ndf=rep(NA, n.mdls), diffuse=rep(NA, n.mdls),
    marginal=rep(NA, n.mdls), 
    rmse.lnUSD=rep(NA, n.mdls), 
    rmse.pbtUnemp=rep(NA, n.mdls), 
    logScale=character(n.mdls), 
    constr=character(n.mdls), 
    reference=character(n.mdls), 
    p.value=character(n.mdls), 
    row.names=1:n.mdls, stringsAsFactors=FALSE)
#
logLiks[1, 2:6] <- logLik2(EWMA1fit)
logLiks[1, 7:8] <- c('y', '-')

#EWMA1.rstd <- rstandard(EWMA1.KFS)

depr1 <- with(GDP, (1927<=Year) & (Year <= 1948))
#GDP[depr1, c('Year', 'realGDPperCapita')]

depr <- with(GDP, (1929<=Year)&(Year<=1947))
EWMA1mdl.woDepr <- EWMA1mdl
EWMA1mdl.woDepr$y[depr] <- NA
EWMA1.woDeprFit<-fitSSM(EWMA1mdl.woDepr, 
                         inits=c(lnQ=0, lnH=0))
EWMA1.woDeprPred <- predict(EWMA1.woDeprFit$model, 
              interval='prediction', filtered=TRUE)
```

Do we want to include $T_t$ for the different models in the paper -- or leave it implied in the table and refer the interested reader to the other vignette?  

EWMA2:  
 
$$T_t = \left[\begin{array}
{rrr}
1 & 1 \\
0 & 1 
\end{array}\right]
$$  




The p.value in the table was obtained by suing [Wilks's theorem](https://en.wikipedia.org/wiki/Likelihood-ratio_test#Distribution:_Wilks.27s_theorem) to compare the marginal likelihood of the model in that row with the reference model.  The likelihood ratio in this test is "most powerful", per the [Neyman–Pearson lemma](https://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma).

However, the chi-square approximation used to compute the p.value displayed here may be low by a factor of 2 or more if the parameter tested is on a boundary, e.g., with an estimated variance = 0 or a correlation = (-1). 
JOUNI:  What do you suggest we do with this?  I vote we publish a the naive Wilk's p-values, noting as follows:  
Unfortunately, Wilk's theorem does not apply when the null hypothesis is on a boundary, as with a variance parameter at 0.  In that case, the likelihood ratio is still most powerful, but twice the log(likelihood ratio) may not be approximately chi-square.  `r Citet(bib2, "PinBates")` recommend simulation to estimate p-values.  They provide simulations that suggest that in cases like these, the p-values per Wilk's theorem tend to be larger than the actual (simulated) p-values by factors ranging from 1 to 3. If that's accurate for these cases, then a nominal p-value of 0.05 means that it likely is significant at that level, which a nominal p-value of 0.10 says that it might be significant at the 0.05 level but not at the 0.02 level, for example.  

```{r, echo=FALSE}
(a1.2 <- c(level=log(GDP$realGDPperCapita[1]), slope=0))
EWMA2fmla <- log(realGDPperCapita)~
  SSMtrend(2,Q=list(Q.lvl=matrix(NA), Q.slope=matrix(NA)), 
           a1=a1.2, ynames='lnGDPperCap')

EWMA2mdl <-SSModel(EWMA2fmla, GDP, H=matrix(NA))

EWMA2init <- c(lnQ.lvl=0, lnQ.slope=0, lnH=0)
EWMA2fit<-fitSSM(EWMA2mdl, inits=EWMA2init)

logLiks[2,  2:6] <- logLik2(EWMA2fit)
logLiks[2, 7:9] <- c('y', '-', 'EWMA1')
logLR2 <- apply(logLiks[1:2, 2:4], 2, diff)
logLiks[2, 10] <- signif(pchisq(2*logLR2[3], 2, 
                         lower=FALSE), 2)
```

The RMSE at 0.043 is only slightly smaller than the root mean square prediction error of 0.046 for the standard Bayesian EWMA discussed above.  However, the likelhood ratio p-value is quite small indicating this difference in highly signficant.  

Let's continue as above by examining the estimated migration and observation error standard deviations:  

```{r}
exp(EWMA2fit$optim.out$par/2)
exp(EWMA1fit$optim.out$par/2)
```

While H is at a boundary, it was not one of the two parameters tested on the comparison of EWMA1 and EWMA2, which suggests that Wilk's theorem may apply in this case.  

```{r}
EWMA2.KFS <- KFS(EWMA2fit$model)
```


```{r}
EWMA2pres1Q.slope <- array(0, c(1,1,N), 
      dimnames=list('slope', 'slope', GDP$Year))
contin <- duplicated(GDP$executive)
presLastYr <- c(!contin[-1], FALSE)
EWMA2pres1Q.slope[,,presLastYr] <- NA 
```

We now create a formula for this model:  

```{r}
EWMA2pres1fmla <- log(realGDPperCapita) ~ 
  SSMtrend(2, Q = list(lnQ.lvl=matrix(NA), 
              lnQ.slope=EWMA2pres1Q.slope), a1=a1.2,
           ynames='lnGDPperCap') 

EWMA2pres1mdl <-SSModel(EWMA2pres1fmla, GDP, H=matrix(NA))

EWMA2pres1updt <- function(pars=EWMA2init,
              model=EWMA2pres1mdl, ...){
  vars <- exp(pars)
  Q <- model$Q
  Q[1,1,] <- vars[1]
  Q[2,2,presLastYr] <- vars[2]
  model$Q <- Q
  model$H[] <- vars[3]
  model
} 

EWMA2pres1fit <- fitSSM(EWMA2pres1mdl, 
      inits=EWMA2init, updatefn=EWMA2pres1updt)

logLiks[3, 2:6] <- logLik2(EWMA2pres1fit)

logLiks[3, 7:9] <- c('y', '-', 'EWMA2')
logLR2p1 <- apply(logLiks[2:3, 2:4], 2, diff)
logLiks[3, 10] <- 1
```

Before proceding with this, let's revise the updatefn to use constrained optimization.  `r Citet(bib2, "lme4vignette")` note that "the nlme package ... goes to some lengths to use an unconstrained variance-covariance parameterization (the log-Cholesky parameterization ...) [which] often give[s] rise to convergence warnings as the algorithm tries to find a maximum on an asymptotically flat surface."  The newer lme4 package prefers constrained optimization using a [Cholesky (upper triangular standard deviation matrix)](https://stat.ethz.ch/R-manual/R-devel/library/base/html/chol.html) parameterization.  This has multiple advantages:  A standard quadratic approximation to the log(likelihood) will tend to be more accurate.  This in turn will tend to reduce convergence problems and increase the utility of likelihood ratio confidence itervals.  In addition, standard deviations are in the units of the response (log dollars in this case, not log-log dollars).  

```{r EWMA2p1Updt2}
EWMA2init2 <- exp(EWMA2init)
names(EWMA2init2) <- c('Q.lvl', 'Q.slope', 'H')
EWMA2pres1updt2 <- function(pars=EWMA2init2,
              model=EWMA2pres1mdl, ...){
  Q <- model$Q
  Q[1,1,] <- pars[1]
  Q[2,2,presLastYr] <- pars[2]
  model$Q <- Q
  model$H[] <- pars[3]
  model
} 
EWMA2pres1fit2 <- fitSSM(EWMA2pres1mdl,
    inits=EWMA2init2, updatefn=EWMA2pres1updt2,  
    method = "L-BFGS-B", lower=rep(0, 3), 
    upper=rep(Inf, 3))
EWMA2pres1fit2$optim.out
EWMA2pres1fit$optim.out
logLiks[3, 8] <- 'failed'
```

```{r}
EWMA2p1init <- exp(EWMA2pres1fit$optim.out$par)
names(EWMA2p1init) <- c('Q.lvl', 'Q.slope', 'H')
EWMA2p1init
EWMA2pres1fit2a <- fitSSM(EWMA2pres1mdl,
    inits=EWMA2p1init, updatefn=EWMA2pres1updt2,  
    method = "L-BFGS-B", lower=rep(0, 3), 
    upper=rep(Inf, 3))
EWMA2pres1fit2a$optim.out
```

This stopped where it started with an ABNORMAL_TERMINATION_IN_LNSRCH -- but without violating the lower bound as before.  

`r Citet(bib2, "lme4vignette")` recommend bounded optimization by quadratic approximation ([BOBYQA](http://rpackages.ianhowson.com/cran/minqa/man/bobyqa.html)) in the [minqa](https://rweb.crmda.ku.edu/cran/web/packages/nloptr/minqa.pdf) package or Nelder-Mead, BOBYQA, or COBYLA in the [nloptr](https://rweb.crmda.ku.edu/cran/web/packages/nloptr/nloptr.pdf) package.  

To use this, we must write our own objective function (modeled after the content of fitSSM):  

```{r likfn}
likfn <- function(pars=EWMA2init2, model=EWMA2pres1mdl, 
                  updatefn=EWMA2pres1updt2) {
    Model <- do.call(updatefn, args = 
                       list(pars, model))
    (-logLik(object = Model, check.model = FALSE))
}
likfn()
likfn(EWMA2p1init)

library(minqa)
(EWMA2pres1fit2a <- bobyqa(EWMA2init2, likfn, 
        lower=rep(0, 3), model=EWMA2pres1mdl, 
        updatefn=EWMA2pres1updt2))
```

This stopped with Objective = (-272.68), substantially larger than the previous min of (-386.91).  Let's try starting with the previous best:  

```{r EWMA2p1}
(EWMA2pres1fit2b <- bobyqa(EWMA2p1init, likfn, 
        lower=rep(0, 3), model=EWMA2pres1mdl, 
        updatefn=EWMA2pres1updt2))
```

This put Q.slope = H = 0, their lower bounds, with only a very minor change in Q.lvl and essentially no change in the objective function.  

Just for completeness, let's try nloptr:  

```{r nloptr}
library(nloptr)
(EWMA2pres1fit2c <- nloptr(EWMA2init2, likfn, 
        lb=rep(0, 3), 
        opts=list(algorithm='NLOPT_LN_BOBYQA'), 
        model=EWMA2pres1mdl, updatefn=EWMA2pres1updt2))
(EWMA2pres1fit2d <- nloptr(EWMA2p1init, likfn,
        lb=rep(0, 3), 
        opts=list(algorithm='NLOPT_LN_BOBYQA'), 
        model=EWMA2pres1mdl, updatefn=EWMA2pres1updt2))

(EWMA2pres1fit2e <- nloptr(EWMA2init2, likfn, 
        lb=rep(0, 3), 
        opts=list(algorithm='NLOPT_LN_NELDERMEAD'), 
        model=EWMA2pres1mdl, updatefn=EWMA2pres1updt2))
(EWMA2pres1fit2f <- nloptr(EWMA2p1init, likfn,
        lb=rep(0, 3), 
        opts=list(algorithm='NLOPT_LN_NELDERMEAD'), 
        model=EWMA2pres1mdl, updatefn=EWMA2pres1updt2))
```

Conclusion:  We got convergence when we gave the answer but not when we didn't.  That doesn't help.  

We could experiment with more algorithms and starting values, but the best solutions so far match the original solution. 

We'll compute the smoothed estimate of the state vector, because we use it in the next model:  

```{r}  
EWMA2pres1.KFS <- KFS(EWMA2pres1fit$model)
```

# 4.  Assuming no continuity in slope between executives 

EWMA2pres2:  

$$T_t = \left[\begin{array}
{rrr}
1 & 1 & 0 \\
0 & \gamma_t & (1-\gamma_t) \\
0 & 0 & 1 
\end{array}\right], 
$$  

where $\gamma_t$ = 0 for the last year of a presidency, and 1 otherwise.  And $Q_t$ is as follows:  

$$Q_t = \left[\begin{array}
{rrr}
q_{\mathrm{lvl}} & 0 & 0 \\
0 & q_{\mathrm{pres2}} (1-\gamma_t) & 0 \\
0 & 0 & 0 
\end{array}\right], 
$$  

where $q_{\mathrm{lvl}}$ is the migration variance for the level, and $q_{\mathrm{pres2}}$ = $P_{1,2,2}$ is the variance of the presidential effect;  $P_{1,3,3}$ must be 0, matching $Q_{3,3,t}$ for all $t$.  And $P_{1,\infty}$ is a diagonal matrix with 1 in the first position and zeros elswhere to indicate a diffuse prior for the level but not the slope.  

This makes $\alpha_{2,t+1} = \alpha_{3,t} + \eta_t$ for $t$ = the last year of each presidency (and $t+1$ = the first year of the next).  For continuing years of a presidency, $\alpha_{2,t+1} = \alpha_{2,t}$, because $Q_{2,2,t}$ = 0 for those years.  

And making $Q_{3,3,t}$ = 0 for all $t$ means that $\alpha_{3,t}$ is normal with mean $a_{1,3}$ and the indicated presidential effect variance, $q_{\mathrm{pres2}}$.    

Similar to the EWMA2 above, we start by supplying a formula using [SSMtrend(3, ...)](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) -- specifying the degree of the "trend" as 3 to give us a 3-dimensional state vector.  We want to estimate $a_{1,\mathrm{pres2}}$ = $a_{1,2}$ = $a_{1,3}$.  KFAS protocol assigns NA to numbers to be estimated.  And we can use the same Q.slope as with EWMA2pres1:      

```{r}
(a1.3 <- c(level=log(GDP$realGDPperCapita[1]), 
           slope=NA, a1.slope=NA))
EWMA2pres2.P1inf <- diag(c(1,0,0))
dimnames(EWMA2pres2.P1inf) <- list(names(a1.3), names(a1.3))
EWMA2pres2.P1 <- EWMA2pres2.P1inf
EWMA2pres2.P1[1,1] <- 0
EWMA2pres2.P1[2,2] <- NA
EWMA2pres2fmla <- log(realGDPperCapita)~
  SSMtrend(3,Q=list(Q.lvl=matrix(NA),
                    Q.slope=EWMA2pres1Q.slope, 
                    Q.a1.slope=matrix(0)), 
           a1=a1.3, P1=EWMA2pres2.P1, 
           P1inf=EWMA2pres2.P1inf, 
           ynames='lnGDPperCap')

EWMA2pres2mdl. <-SSModel(EWMA2pres2fmla, GDP,
                         H=matrix(NA))
```

[SSModel](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) does not remember the names of the states we assigned to a1.3 above. If we want those names to appear later, we can fix them now;  we'll write a function to do that:  

```{r}
fixModelNames <- function(model, stateNames, 
                          yNames){
  mdl <- model 
  if(missing(yNames)){
    yNames <- colnames(mdl$y)
    if(is.null(yNames)) {
      yNames <- dimnames(mdl$Z)[[1]]
    }
  }
  colnames(mdl$y) <- yNames
  mdlTime <- time(mdl$y)
  n <- attr(mdl, 'n')
  if(length(mdlTime)<n){
    mdlTime <- dimnames(mdl$T)[[3]]
    if(length(mdlTime)<n){
      mdlTime <- dimnames(mdl$Q)
      if(length(mdlTime)<n){
        mdlTime <- dimnames(mdl$H)
        if(length(mdlTime)<n){
          mdlTime <- 1:n
        }
      }
    }
  }
# Z
  if(dim(mdl$Z)[3]>1){
    dimnames(mdl$Z) <- list(yNames, stateNames, 
                            mdlTime)
  } else {
    dimnames(mdl$Z) <- list(yNames, stateNames, NULL)
  }
# H
  if(dim(mdl$H)[3]>1){
    dimnames(mdl$H) <- list(yNames, yNames, 
                            mdlTime)
  } else {
    dimnames(mdl$H) <- list(yNames, yNames, NULL)
  }
# T   
  if(dim(mdl$T)[3]>1){
    dimnames(mdl$T) <- list(stateNames, stateNames, 
                            mdlTime)
  } else {
    dimnames(mdl$T) <- list(stateNames, stateNames,
                            NULL)
  }
# R  
  if(dim(mdl$R)[3]>1){
    dimnames(mdl$R) <- list(stateNames, stateNames, 
                            mdlTime)
  } else {
    dimnames(mdl$R) <- list(stateNames, stateNames,
                            NULL)
  }
# Q  
  if(dim(mdl$Q)[3]>1){
    dimnames(mdl$Q) <- list(stateNames, stateNames, 
                            mdlTime)
  } else {
    dimnames(mdl$Q) <- list(stateNames, stateNames,
                            NULL)
  }
# a1, P1, P1inf 
  dimnames(mdl$a1)[[1]] <- stateNames 
  dimnames(mdl$P1) <- list(stateNames, stateNames)
  dimnames(mdl$P1inf) <- list(stateNames, stateNames)
#
  mdl 
}
EWMA2pres2mdl <- fixModelNames(EWMA2pres2mdl., 
            names(a1.3), 'logGDPperCapita' )
```

Next, we modify this model to make T time-varying as desired:   

```{r}
T3.other <- EWMA2pres2mdl$T
EWMA2pres2.T <- array(T3.other, c(3,3,N), 
      dimnames=dimnames(T3.other))
dimnames(EWMA2pres2.T)[[3]] <- GDP$Year
EWMA2pres2.T[2,2,presLastYr] <- 0
EWMA2pres2.T[2,3,] <- presLastYr

EWMA2pres2mdl$T <- EWMA2pres2.T

attr(EWMA2pres2mdl, 'tv') <- c(Z=0L, H=0L, 
                        T=1L, R=0L, Q=1L)
```

Now let's talk about unknown parameters to estimate:  This model requires us to estimate one parameter more than EWMApres1:  $a_{1,\mathrm{pres2}}$ = $a_{1,2}$ = $a_{1,3}$:  What initial values should we use for these parameters?  We can set a1.2 to the average slope from EWMA2pres1fit:  

```{r}
EWMA2pres2.a1.2 <- mean(EWMA2pres1.KFS$alphahat[,2])
```

The other three parameters are crudely similar to the three paramters estimates for EWMA2pres1:

```{r}
EWMA2pres1fit$optim.out$par
exp(EWMA2pres1fit$optim.out$par/2)
```

Three variances were estimated on the log scale, so exponentiating half those numbers gives us the corresponding standard deviations.  The migration standard deviation for the level was just over 0.04.  The other two were much smaller.  Let's use the largest of the three as the starting value for all three:  

```{r}
EWMA2pres2init <- c(a1.2=EWMA2pres2.a1.2,
        EWMA2pres1fit$optim.out$par)
EWMA2pres2init[3:4] <- EWMA2pres2init[2]
EWMA2pres2init
```

Let's modify the previous updatefn to do what we want for this model:  

```{r}
EWMA2pres2updt <- function(pars=EWMA2pres2init, 
                      model=EWMA2pres2mdl, ...){
#  a1 
  model$a1[2:3] <- pars[1]
#  Q  
  vars <- exp(pars[-1])
  Q <- model$Q
  Q[1,1,] <- vars[1]
  Q[2,2,presLastYr] <- vars[2]
  model$Q <- Q
# P1 
  model$P1[2,2] <- vars[2]
# H   
  model$H[] <- vars[3]
  model
} 
```

We now fit this model as follows:  

```{r}
EWMA2pres2fit <- fitSSM(EWMA2pres2mdl, 
          inits=EWMA2pres2init, updatefn=EWMA2pres2updt) 

logLiks[4, 2:6] <- logLik2(EWMA2pres2fit)

logLiks[4, 7:9] <- c('y', '-', 'EWMA2')
logLR2p2 <- apply(logLiks[c(2,4), 2:4], 2, diff)
logLiks[4, 10] <- 'significant'

logLiks[1:4,]
```

This has a much better fit, as indicated by the increase in the log(likelihoods) and reduction in rmse.

We next compute and plot the smoothed estimate of the state vector, as before:  

```{r}
EWMA2pres2.KFS <- KFS(EWMA2pres2fit$model)
```

This strongly suggests that the Hoover administration was a statistical outlier, while FDR (with World War II) may or may not have been.  To make it easier to compare administrations, let's write a function to extract the growth associated with each president and its variance:  

```{r}
stateByFactor <- function(object, state='slope', 
              factor.=GDP$executive, cor.=1, 
              conf=0.95, factorName='president'){
# object has has components alphahat and V, 
#    because it is assumed to be of class KFS   
# mean state 
  stateNames <- colnames(object$alphahat)
  if(is.null(stateNames)){
    stop('alphahat needs colnames; absent')
  } 
  ist <- which(colnames(object$alphahat) == state)
  if(length(ist) != 1){
    stop('state = ', state, ' not found in ', 
         'colnames(alphahat) = ', 
         paste(stateNames, collapse=', '))
  }
#
  meanState <- mean(object$alphahat[, state])
  facState <- tapply(object$alphahat[, state], 
                      factor., mean)
  nSt <- length(facState)
# If the correlations are all 1, the standard deviation 
# of an average is the average of the standard deviations.
  meanVar. <- tapply(object$V[ist,ist,], factor., mean)
  meanSD. <- tapply(sqrt(object$V[ist,ist,]), 
                      factor., mean)
  meanVar <- (1-cor.)*meanVar.+cor.*(meanSD.^2)
  meanSD <- sqrt(meanVar)
#
  tabfac <- table(factor.)
  t. <- (facState-meanState)/meanSD 
  p. <- 2*pnorm(-abs(t.))
# variance estimated using all the data, 
# not just those within each presidency, 
# so dft = (total obs) - nSt
  dfe <- (nrow(object$alphahat) - nSt)
  st2 <- qt(1-.5*(1-conf), dfe) * meanSD
  st2. <- st2 %o% c(lower=-1, upper=1)
  confInt <- as.numeric(facState) + st2.
#
  Exec <- ordered(names(facState), names(facState))
  stateSum <- cbind(data.frame(Exec=Exec, 
          n=as.numeric(tabfac),
          mean=as.numeric(facState), 
          stdError=as.numeric(meanSD),
          t.=as.numeric(t.), 
          p_value=as.numeric(p.)), confInt)
  names(stateSum)[c(1, 3)] <- c(factorName, state)
# 
  chisq. <- sum(t.^2)
  F.. <- (chisq./(nSt-1))
  F. <- c(F=F.., df1=nSt-1, df2=dfe, 
          p=pf(chisq., nSt, dfe, lower=FALSE))
  print(F.)
  attr(stateSum, 'F') <- F.
#
  stateSum
}

EWMA2pres2sum <- stateByFactor(EWMA2pres2.KFS)
```

The smallest p_value here is 1.3e-6 = 1.3 chances in a million for FDR.  [Bonferroni's rule](https://en.wikipedia.org/wiki/Bonferroni_correction) tells us to multiple the smallest p_value by the number of tests conducted, 41.  When we do that, we get 5e-5 = 5 chances in a hundred thousand. That's clearly statistically signficant.  The second smallest p_value is 1.5e-04 = 0.00015 for Hoover.  His administration is also clearly different from the rest of US economic history.  No one else seems different by these measures.  

Let's plot the confidence intervals just computed using [ggplot](http://rpackages.ianhowson.com/cran/ggplot2/man/ggplot.html):    

```{r,fig.cap = "Decoupled slope for each presidency"}
suppressMessages(library(ggplot2))

ggplot(EWMA2pres2sum[
    c('president', 'slope', 'lower', 'upper')], 
       aes(x=president, y=slope)) + geom_point() +
    coord_cartesian(ylim = c(-0.08, 0.08)) + 
    geom_errorbar(ymin=EWMA2pres2sum$lower,
                  ymax=EWMA2pres2sum$upper) + 
    geom_hline(aes(yintercept=mean(slope),color="red")) # mean slope
```  

This clearly says that the Hoover and FDR administrations were different from all others, but it still doesn't say whether FDR himself was different or World War II created the difference.  Before considering that, let's continue with the other alternative presidency model.  

One more question:  What were the parameter estimates?

```{r}
EWMA2pres2fit$optim.out$par
exp(EWMA2pres2fit$optim.out$par[-1]/2)
```

This says that the average growth rate is 1.6 percent per year, the standard deviations of the migration for level and slope are 0.038 and 0.019, respectively, and the standard deviation of the observation error is tiny, 0.00004.  

Before looking at the next model, let's review the standardized residuals as before:  

```{r}
EWMA2pres2.rstd <- rstandard(EWMA2pres2.KFS)
```

What do we get by modeling a president effect as an adjustment to the slope of an otherwise standard Bayesian double EWMA?

# 5.  EWMA2 with an adjustment for each president 

This model considers a 3-dimensional state vector:  level, slope and a Presidential adjustment for slope.  For this, we will want a state transition matrix like the following:  

$$T_t = \left[\begin{array}
{rrr}
1 & 1 & 1 \\
0 & 1 & 0 \\
0 & 0 & \gamma_t 
\end{array}\right],
$$ 

where $\gamma_t$ = 0 for the last year of a presidency, and 1 otherwise, as with the immediately preceding model, EWMA2pres2.  

We combine this with migration variance $Q_t$ as follows:  

$$Q_t = \left[\begin{array}
{rrr}
q_{\mathrm{lvl}} & 0 & 0 \\
0 & q_{\mathrm{slope}} & 0 \\
0 & 0 & q_{\mathrm{pres3}} (1-\gamma_t) \\
\end{array}\right], 
$$  

where $q_{\mathrm{lvl}}$ is the migration variance for the level (as before), $q_{\mathrm{slope}}$ is the migration variance of the slope, $q_{\mathrm{pres3}}$ = $P_{1,3,3}$ is the variance of the presidential effect.  The rest of $P_1$ is 0, and $P_{1,\infty}$ is a diagonal matrix with elements (1, 1, 0) indicating a diffuse prior for level and slope but not the presidential effect, which we estimate.  

Let's start by modifying the formula, similar to what we did with EWMA2pres1fmla, above, but for this 3-dimensional state    

```{r}
a1.3a <- c(level=log(GDP$realGDPperCapita[1]), slope=0, 
           pres3=0)
EWMA2pres3P1 <- diag(c(0, 0, NA))
EWMA2pres3P1inf <- diag(c(1, 1, 0))
EWMA2pres3fmla <- log(realGDPperCapita) ~ 
  SSMtrend(3, Q = list(level=matrix(NA), slope=matrix(NA),
          pres3=EWMA2pres1Q.slope), 
      P1=EWMA2pres3P1, P1inf=EWMA2pres3P1inf, a1=a1.3a, 
      ynames='logGDPperCap') 

EWMA2pres3mdl. <- SSModel(EWMA2pres3fmla, GDP, H=matrix(NA))
EWMA2pres3mdl <- fixModelNames(EWMA2pres3mdl., 
                    names(a1.3a), 'logGDPperCap' )

pres3states <- names(a1.3a)
pres3T <- array(EWMA2pres3mdl$T, dim=c(3,3,N), 
        dimnames=list(pres3states, pres3states, GDP$Year) )
pres3T[1,3,] <- 1
pres3T[2,3,] <- 0 
pres3T[3,3, presLastYr] <- 0

EWMA2pres3mdl$T <- pres3T 

attr(EWMA2pres3mdl, 'tv') <- c(Z=0L, H=0L, 
                               T=1L, R=0L, Q=1L)

EWMA2pres2fit$optim.out$par
EWMA2pres3init <- rep(
        EWMA2pres2fit$optim.out$par[2], 4)
names(EWMA2pres3init) <- c("lnQ.lvl", "lnQ.slope", 
        "lnQ.pres3", "lnH")
EWMA2pres3init

EWMA2pres3updt <- function(pars=EWMA2pres3init, 
                      model=EWMA2pres3mdl, ...){
  vars <- exp(pars)
  Q <- model$Q
  Q[1,1,] <- vars[1]
  Q[2,2,] <- vars[2]
  Q[3,3,presLastYr] <- vars[3]
  model$Q <- Q
  model$P1[3,3] <- vars[3]
  model$H[] <- vars[4]
  model
}

EWMA2pres3fit <- fitSSM(EWMA2pres3mdl,
                    inits=EWMA2pres3init, 
                        updatefn=EWMA2pres3updt) 

logLiks[5, 2:6] <- logLik2(EWMA2pres3fit)

logLiks[5, 7:9] <- c('y', '-', 'EWMA2')
logLR2p3 <- apply(logLiks[c(2, 5), 2:4], 2, diff)
logLiks[5, 10] <- signif(pchisq(2*logLR2p3[3], 1, 
                         lower=FALSE), 2)
logLiks[1:5,]
```

The diffuse likelihood and the root mean square error are worse for EWMApres3 than for EWMApres2, but the marginal likelihood is better. The theory says we should trust the marginal likelihood.  However, the root mean square of the one-step ahead prediction errors is worse with the better marginal likelihood.  

```{r}
EWMA2pres3.KFS <- KFS(EWMA2pres3fit$model)
```

*** JOUNI:  IF YOU READ THIS FAR, PLEASE NOTE THAT THE PRIMARY THING I THINK WE WANT IN THIS PROCEEDINGS PAPER FROM ALL THIS IS THE SUMMARY TABLE.  

*** I CAN FINISH THIS NEXT WEEK IF YOU AGREE IT'S WORTH DOING.  


This is interesting:  Let's check the parameter estimates and the ranges of of the three components of the state vector:  

```{r}
EWMA2pres3fit$optim.out$par
exp(EWMA2pres3fit$optim.out$par/2)
apply(EWMA2pres3.KFS$alphahat, 2, range)
```

The slope is estimated as a constant, making this model virtually equivalent to EWMA2pres2.  

What does the presidential summary look like? 

```{r}
(EWMA2pres3sum <- stateByFactor(EWMA2pres3.KFS, "pres3"))
```

This seems quite similar to the EWMA2pres2 model.  Let's plot this summary as before:  

```{r,fig.cap = "EWMA2pres3 slope for each presidency"}
ggplot(EWMA2pres3sum[
          c('president', 'pres3', 'lower', 'upper')], 
       aes(x=president, y=pres3)) + geom_point() +
    coord_cartesian(ylim = c(-0.08, 0.08)) + 
    geom_errorbar(ymin=EWMA2pres3sum$lower,
                  ymax=EWMA2pres3sum$upper) + 
    geom_hline(aes(yintercept=mean(pres3),color="red")) 
```

This is quite similar to the corresponding plot for EWMA2pres2.  These two models support the same basic conclusion:  There is a president effect, the Hoover administration performed distinctly worse than the others and the FDR administration performed distinctly better.  

However, there are difference between the two that are not easy to understand or describe.  EWMA2pres3 has a better marginal likelihood but worse root mean square prediction error.  

Let's look at the standardized residuals as before:  

```{r,fig.cap = "Autocorrelation Function of Standardized Residuals from EWMA2pres3"} 
EWMA2pres3.rstd <- rstandard(EWMA2pres3.KFS)
str(EWMA2pres3.rstd)
(acf(EWMA2pres3.rstd, na.action=na.pass, demean=TRUE))
```

This is essentially the same as EWMA2pres2.  

Let's again look at a normal probability plot of these residuals:  

```{r,fig.cap = "Autocorrelation Function of Standardized Residuals"} 
qqnorm(EWMA2pres3.rstd, datax=TRUE, 
       ylab='standardized residuals', 
       xlab='standard normal quantiles')
```

As before.  The continued presence of the image of a mixture of normals suggests an opportunity to improve the fit through improving the observation noise model, e.g., with [Autoregressive conditional heteroskedasticity](https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity).  We will not pursue that here.  

NOTE:  An equivalent model estimating the same parameters could have a 43-dimensional state vector with components level, slope, and Pres1, ..., Pres41.  The memory and compute time required to fit it would be greater, but the [logLik](http://rpackages.ianhowson.com/cran/KFAS/man/logLik.SSModel.html) and parameter estimates should be the same.  This is unnecessary here but could be required with, e.g., an experiment with levels for a random effect that are not sequential in time.  

# 6.  EWMA2 with a fixed effect for war 

The plots discussed above suggest that if war has a macroeconomic effect, it increases the rate of economic growth.  This suggests we model the impact of war as an adjustment to slope for incrementing level within the state transition matrix, $T_t$.  It further suggests we want a four-dimensional state vector, with one component ($w_0$) for the presence ($w_0$ = 1) or absence ($w_0$ = 0) of war and the second proportional to battle deaths per million population ($w_1$):

$$T_t = \left[\begin{array}
{rrrr}
1 & 1 & w_0 & w_1 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 
\end{array}\right],
$$ 

For this model, Q is constant, and we want a diffuse prior on all 4 components of the initial state vector.  Thus, $P_{1,\infty}$ is the 4-dimensional identity matrix, and $P_1$ is the 4-dimensional zero matrix.  

Thus, for this model, we do not need to create a time-varying Q nor an updatefn as we did for models 3-5.  This simplifies the modeling process to the following:  

<center> 
[formula](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html) -> [SSModel](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) -> 

[(time-varying state transition matrix, T)](http://rpackages.ianhowson.com/cran/KFAS/man/KFAS.html) -> [fitSSM](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) -> [KFS](http://rpackages.ianhowson.com/cran/KFAS/man/KFS.html)  
</center>

We can model war as a fixed effect multiple ways.  One is to use $R_t$ = the 4-dimensional identity matrix as before, with $Q_t$ being a diagonal matrix with only the first two elements nonzero.  Another is to let $R_t$ be the first two columns of a 4-dimensional identity matrix and estimate the diagonal of the resulting 2 x 2 $Q_t$ matrix.  We'll use the former here, because the next model with war as a random effect requires $R_t$ = the 4-dimensional identity matrix.  

We'll use a similar formula to those above, requesting a fourth degree [SSMtrend](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) and modifying $T_t$ as before: 

```{r}
a1.4 <- c(level=log(GDP$realGDPperCapita[1]), slope=0, 
           war=0, deaths=0)
EWMA2war0fmla <- log(realGDPperCapita) ~ 
    SSMtrend(4, Q = list(level=matrix(NA), slope=matrix(NA),
                  war=matrix(0), deaths=matrix(0)), a1=a1.4,
           ynames='lnGDPperCap') 
EWMA2war0mdl. <- SSModel(EWMA2war0fmla, GDP, H=matrix(NA))
```

As before, let's fix the names of components of EWMAwar0mdl:  

```{r}
EWMA2war0mdl <- fixModelNames(EWMA2war0mdl., 
                  names(a1.4), 'logGDPperCap' )
```

Now we need to create the desired time-varying state transition matrix, $T_t$:  

```{r}
warStates <- names(a1.4)
war0T <- array(diag(4), dim=c(4,4,N), 
    dimnames=list(warStates, warStates, GDP$Year))
war0T[1, 2,] <- 1  
war0T[1, 3,] <- (GDP$war!='')
war0T[1, 4,] <- GDP$battleDeathsPMP
war0T[,,1:9]
```

Let's check this $T_t$ with years at war and peace.  

```{r}
GDP[c(1, 2, 7), c('Year', 'war', 'battleDeathsPMP')]
war0T[,,c(1, 2, 7)]
```

They match.  Let's save this T as part of EWMA2war0mdl:  

```{r}
EWMA2war0mdl$T <- war0T 
```

We also need to fix the 'tv' attribute, as we did for models 4 and 5 (EWMA2pres2 and EWMA2pres3):  

```{r}
attr(EWMA2war0mdl, 'tv') <- c(Z=0L, H=0L, T=1L, R=0L, Q=0L)
```

This model has the same 3 parameters to estimate as models 2, 3 and 4 above:  lnQ.lvl, lnQ.slope, and lnH.  We will use the same initial values:  EWMA2init.

```{r}
(EWMA2war0fit <- fitSSM(EWMA2war0mdl, inits=EWMA2init)) 
logLiks[6, -1] <- logLik2(EWMA2war0fit)
logLiks[1:6,1:5]
```

Conclusion:  

  1.  If we use the marginal likelihood rather than the diffuse likelihood, it says that EWMA2war0 gives a slightly better fit than EWMA2:  396.9705 vs. 396.5067.  

  1.  However, the improvement in the fit is so slight, it's clearly not statistically significant.  
  
Let's try starting with the final parameter estimates of EWMA2:

```{r}
EWMA2war0fit2 <- fitSSM(EWMA2war0mdl, 
                       inits=EWMA2fit$optim.out$par)
logLik2(EWMA2war0fit2)
```

The same as before.  

As a further check on the code we used, we should get the same answer as from EWMA2 if we set the war components of P1inf to 0:  

```{r}
EWMA2war0mdl3 <- EWMA2war0mdl 
war0P1inf <- diag(c(1, 1, 0, 0))
dimnames(war0P1inf) <- list(warStates, warStates)
EWMA2war0mdl3$P1inf <- war0P1inf
EWMA2war0fit3 <- fitSSM(EWMA2war0mdl3, inits=EWMA2init)
logLik2(EWMA2war0fit3)
```

This gives the same [logLik](http://rpackages.ianhowson.com/cran/KFAS/man/logLik.SSModel.html) and rmse as EWMA2, as we expected.  

At least we can match the diffuse and marginal likelihoods plus rmse.lnUSD of EWMA2.  Let's declare convergence and consider a random effect for war:  

# 7. EWMA2 with a random effect for war 

This model combines EWMA2pres3 with EWMA2war0.  We will use a time-varying state transition matrix as follows:    

$$T_t = \left[\begin{array}
{rrrr}
1 & 1 & w_0 & w_1 \\
0 & 1 & 0 & 0 \\
0 & 0 & w_0 & 0 \\
0 & 0 & 0 & w_0  
\end{array}\right],
$$ 

where $w_0$ = 1 during wars and 0 otherwise, as above.  And we need to make the migration variance time-varying to match: 

<center> 
[(time-varying migration variance, Q)](http://rpackages.ianhowson.com/cran/KFAS/man/KFAS.html) -> [formula](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html) -> [SSModel](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) -> 

[(state transition matrix, T)](http://rpackages.ianhowson.com/cran/KFAS/man/KFAS.html) -> [(updatefn)](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) -> [fitSSM](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) -> [KFS](http://rpackages.ianhowson.com/cran/KFAS/man/KFS.html)  
</center>

Similar to EWMA2pres2 and EWMA2pres3, we will allow Q for the (war, deaths) components of the state vector to be nonzero the year before the start of each war:   

```{r}
(beforeWar <- which(c(
      head(GDP$war, -1)=='' & tail(GDP$war, -1)!=''), FALSE))
```

Let's check:  

```{r}
GDP[beforeWar[1]+0:1,]
```

We want Q[1:2, 1:2, ] to be diag(c(NA, NA)), Q[3:4, 3:4, beforeWar] to NA;  all other elements of Q should be 0:  

```{r}
EWMA2warQ <- array(0, c(4,4,N), 
    dimnames=list(warStates, warStates, GDP$Year))
EWMA2warQ[1,1,] <- NA
EWMA2warQ[2,2,] <- NA
EWMA2warQ[3:4,3:4,beforeWar] <- NA 
```

Let's confirm we got what we intended:  

```{r}
EWMA2warQ[,,beforeWar[1]+(-1):1]
```

[SSMtrend(4, ...)](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) requires Q to be a list of length 4, so we can't use that. Instead, we'll copy the previous [SSModel](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) and modify Q directly:  

```{r}
EWMA2war1mdl <- EWMA2war0mdl
EWMA2war1mdl$Q <- EWMA2warQ
attr(EWMA2war1mdl, 'tv')[5] <- 1L
```

Since we are estimating P1[3:4, 3:4] = Q[3:4, 3:4, beforeWar], we need to set P1inf to diag(c(1,1,0,0)):  

```{r}
EWMA2war1mdl$P1inf <- diag(c(1,1,0,0))
```

We also want to set T[3:4, 3:4, GDP$war==''] to 0:    

```{r}
war1T <- war0T
war1T[3:4, 3:4, GDP$war==''] <- 0
war1T[,,c(1:2, 8, 22:24)]
```

Looks good.  Let's put it in the model:  

```{r}
EWMA2war1mdl$T <- war1T
```

We also need a matching updatefn.  That requires specifying the parameters to be estimated, for which we will ultimately need starting values.  For this, we'll want to include 3 parameters for the [nlme::pdLogChol](http://rpackages.ianhowson.com/cran/nlme/man/pdLogChol.html) representation of Q[3:4, 3:4].  We'll call these parameters lnQ.war, lnQ.deaths, and lnQ.warDeaths.  However, since the first two are more like standard deviations than variances, we'll adjust their starting values accordingly.  And the lnQ.warDeaths is more like a regression coefficient, and we'll start that at 0:  

```{r}
EWMA2war1init <- 
    c(1, 1, .5, .5, 0, 1)*rep(EWMA2fit$optim.out$par[1], 6)
names(EWMA2war1init) <- c('lnQ.lvl', 'lnQ.slope', 
      'lnQ.war', 'lnQ.deaths', 'lnQ.warDeaths', 'lnH')
EWMA2war1init
EWMA2war1updt <- function(pars=EWMA2war1init, 
                      model=EWMA2war1mdl, ...){
  Q <- model$Q
  Q[1,1,] <- exp(pars[1])
  Q[2,2,] <- exp(pars[2])
#  
  P1w <- as.matrix(nlme::pdLogChol(pars[3:5]))
  model$P1[3:4, 3:4] <- P1w
  Q[3:4, 3:4, beforeWar] <- P1w
  model$Q <- Q
#  
  model$H[] <- exp(pars[6])
  model
} 
EWMA2war1fit <- fitSSM(EWMA2war1mdl, inits=EWMA2war1init, 
                        updatefn=EWMA2war1updt)
logLiks[1:6,1:5]
logLik2(EWMA2war1fit)[1:4]
```

This looks like more convergence problems with logLik numbers slightly less than those for EWMA2.  

Let's check to confirm we can get logLik(EWMA2fit[['model']]) as before:  

```{r}
EWMA2war1init0 <- EWMA2war1init
EWMA2war1init0[c(1, 2, 6)] <- EWMA2fit$optim.out$par
EWMA2war1init0[3:5] <- c(-999, -999, 0)
EWMA2war1fit0 <- fitSSM(EWMA2war1mdl, inits=EWMA2war1init0, 
                        updatefn=EWMA2war1updt)
logLik2(EWMA2war1fit0)[1:4]
```

Both likelihoods and rmse.lnUSD match EWMA2.  Let's try a constrained standard deviation transformation as we did with EWMA2pres1:  

```{r EWMA2p1aUpdt2}
EWMA2war1init2 <- exp(EWMA2war1init/
        c(2, 2, 1, 1, 1, 2))
EWMA2war1init2[5] <- 0
names(EWMA2war1init2) <- c('Q.lvl', 'Q.slope', 'Q.war', 
    'Q.deaths', 'Q.warDeaths', 'H')
EWMA2war1init2
EWMA2war1updt2 <- function(pars=EWMA2war1init2,
              model=EWMA2war1mdl, ...){
  Q <- model$Q
  Q[1,1,] <- pars[1]^2
  Q[2,2,] <- pars[2]^2
# 
  C1w. <- matrix(c(1, pars[5], pars[5], 1), 2)
  P1w. <- (C1w.*outer(pars[3:4], pars[3:4]))
#  
  model$P1[3:4, 3:4] <- P1w.
  Q[3:4, 3:4, beforeWar] <- P1w.
  model$Q <- Q
#  
  model$H[] <- pars[6]^2
  model
} 

# Confirm that the two functions 
# produce the same model
# from comparable 
EWMA2war1mdl1.0 <- EWMA2war1updt()
EWMA2war1mdl2.0 <- EWMA2war1updt2()
all.equal(EWMA2war1mdl1.0, EWMA2war1mdl2.0)

# lower and upper limits 
EWMA2war1l <- rep(0, 6)
EWMA2war1l[5] <- (-1)
names(EWMA2war1l) <- names(EWMA2war1init2)
EWMA2war1l
EWMA2war1u <- rep(Inf, 6)
EWMA2war1u[5] <- 1
names(EWMA2war1u) <- names(EWMA2war1init2)
EWMA2war1u
EWMA2war1fit2 <- fitSSM(EWMA2war1mdl, inits=EWMA2war1init2, 
    updatefn=EWMA2war1updt2, method = "L-BFGS-B", 
    lower=EWMA2war1l, upper=EWMA2war1u)
logLiks[7, -1] <- logLik2(EWMA2war1fit2)
logLiks[1:7, 1:5]
```

The marginal logLik now matches EWMA2.  

What about war as a variance effect?   

# 8. EWMA2 with variance effect for war

One more possibility is to allow for substantial, single-year changes in the level in EWMA2 by modeling log(Q[1,1]) ~ war + deaths.  For this, we start with EWMA2 and make Q time-varying with an appropriate updatefn.  

```{r}
EWMA2war2mdl <- EWMA2mdl
EWMA2war2Q <- array(EWMA2mdl$Q, dim=c(2,2,N), 
    dimnames=list(names(a1.2), names(a1.2), GDP$Year))
EWMA2war2Q[,, 1:2]
```

This looks OK.  

```{r}
EWMA2war2mdl$Q <- EWMA2war2Q
attr(EWMA2war2mdl, 'tv') <- 
        c(Z=0L, H=0L, T=0L, R=0L, Q=1L)

EWMA2war2init <- c(1, 0, 0, 1, 1)* EWMA2fit$optim.out$par[1]
names(EWMA2war2init) <- c('lnQ.lvl', 'lnQ.lvl.war',
                'lnQ.lvl.deaths', 'lnQ.slope', 'lnH')
EWMA2war2init
```

This looks like plausible starting values.  

```{r}
EWMA2war2updt <- function(pars=EWMA2war2init, 
                      model=EWMA2war2mdl, ...){
#
  Q <- model$Q
  lnQ11 <- (pars[1] + pars[2]*(GDP$war!='') +
                   pars[3]*GDP$battleDeathsPMP)
  Q11 <- exp(lnQ11)
  oops <- which(!is.finite(Q11))
  Q11[oops] <- .Machine$double.xmax
#  
#  noops <- length(oops)
#  if(noops>0){
#    cat('Q[1,1] NA or Inf with pars =\n')
#    print(pars)
#    stop('Problem occurs ', noops, ' times, ', 
#         'starting with number ', noops[1], 
#         ' for which Q[1,1] = ', Q11[oops[1]])
#  }
  Q[1,1,] <- Q11
#
  Q22 <- exp(pars[4])
  if(!is.finite(Q22)){
    print(pars)
    stop('Q22 = ', Q22)
  }
  
  Q[2,2,] <- Q22
  model$Q <- Q
#  
  model$H[] <- exp(pars[5])
  model
} 
EWMA2war2updt()

EWMA2war2fit <- fitSSM(EWMA2war2mdl,
  inits=EWMA2war2init, updatefn=EWMA2war2updt) 
logLik2(EWMA2war2fit)
```

Serios lack of convergence. 

We'd like to try a constrained optimization over standard deviations for this, as with EWMA2war1 above.  However, it's less that obvious how to transform the war effect.  For this, we'll use the following:  

  Q[1,1] = (sQ.lvl^2)*exp(lnQ.lvl.war*war 
                  + lnQ.lvl.deaths*deaths)

```{r}
EWMA2war2init2 <- exp(EWMA2war2init/2)
EWMA2war2init2[2:3] <- 0 
names(EWMA2war2init2) <- c('sQ.lvl', 'lnQ.lvl.war',
                'lnQ.lvl.deaths', 'sQ.slope', 'sH')
EWMA2war2init2

EWMA2war2updt2 <- function(pars=EWMA2war2init2, 
                      model=EWMA2war2mdl, ...){
#
  Q <- model$Q
  Q11 <- ((pars[1]^2)*exp(pars[2]*(GDP$war!='') +
                   pars[3]*GDP$battleDeathsPMP))
  oops <- which(!is.finite(Q11))
  Q11[oops] <- .Machine$double.xmax
#  
#  noops <- length(oops)
#  if(noops>0){
#    cat('Q[1,1] NA or Inf with pars =\n')
#    print(pars)
#    stop('Problem occurs ', noops, ' times, ', 
#         'starting with number ', noops[1], 
#         ' for which Q[1,1] = ', Q11[oops[1]])
#  }
  Q[1,1,] <- Q11
#
  Q22 <- pars[4]^2
  if(!is.finite(Q22)){
    print(pars)
    stop('Q22 = ', Q22)
  }
  
  Q[2,2,] <- Q22
  model$Q <- Q
#  
  model$H[] <- (pars[5]^2)
  model
} 
EWMA2war2updt2()

EWMA2war2fit2 <- fitSSM(EWMA2war2mdl, inits=EWMA2war2init2, 
                        updatefn=EWMA2war2updt2)
logLiks[8, -1] <- logLik2(EWMA2war2fit2)
logLiks[1:8,1:5]
```

This is the best fitting war model so far with a modest increase in the marginal log(likelihood) over EWMA2:  

```{r}
(d2.8 <- apply(logLiks[c(2, 8), 2:5], 2, diff))
pchisq(2*d2.8[3], d2.8[1], lower=FALSE)
```

`r Citet(bib2, "Stiglitz2003")` discussed "The myth of the war economy":  Many people believe that war is good for the economy.  Stiglitz argued the opposite:  "Markets loathe uncertainty and volatility. Conflict brings both."  These data provide no evidence for any war effect -- not even the increased volatility that Stiglitz mentioned.  

What about unemployment?  

# 9.  Modeling Unemployment 

A simple model for unemployment is a standard Bayesian EWMA. This is not adequate, because the older data are clearly not as accurately measured.  

However, before considering that. let's start with this simple model:  

```{r}
unemp1 <- with(GDP, 
               unemployment[!is.na(unemployment)][1])
uEWMA1fmla <- qnorm(unemployment) ~
        SSMtrend(1,Q=list(matrix(NA)),
                 a1 = qnorm(unemp1),
                 ynames='probitUnemp' ) 
```

We use this to create an SSModel as before:   

```{r}
uEWMA1mdl. <-SSModel(uEWMA1fmla, GDP, H=matrix(NA) )
uEWMA1mdl <- fixModelNames(uEWMA1mdl., 
                      'unempLvl', 'probitUnemp')

uEWMA1fit <- fitSSM(uEWMA1mdl, inits=c(lnQ=0, lnH=0))
```

Let's compute one-step ahead predictions with prediction intervals using [predict.SSModel](http://rpackages.ianhowson.com/cran/KFAS/man/predict.SSModel.html):

```{r}
uEWMA1pred <- predict(uEWMA1fit$model, 
              interval='prediction', filtered=TRUE)
```

Let's plot these predictions:  

```{r,fig.cap = "Unemployment and EWMA1"}
ylim <- range(qnorm(GDP$unemployment), na.rm=TRUE)
plotPresWars(qnorm(unemployment), GDP, 
             main="EWMA of qnorm(Unemployment)", ylim=ylim, 
             ylab='unemployment rate', las=1, 
             type='n', xlab='Year', axes=FALSE, 
             yHoover=0.07, yFDR=0.25, 
             yWar=seq(.5, by=-.05, len=9))
text(qnorm(GDP$unemployment), 'x', cex=0.5)
matlines(GDP$Year, uEWMA1pred, 
         col=c('blue', 'darkgreen', 'darkgreen'), 
         lty=c(1, 2, 2))
axis(1)
axis(2, qnorm(unempScale), unempScale)
```

This looks plausible.  What about the log(likelihood)?  

```{r}
logLiks[9, -1] <- logLik2(uEWMA1fit)
logLiks[1:9,]
```

Of course, neither the log-likelihoods nor the root mean square prediction errors for uEWMA1 are comparable to the other models, because the response variable is different.  

However, it's still interesting to look at the estimated migration and observation standard deviations:  

```{r}
exp(uEWMA1fit$optim.out$par/2)
```

These seem consistent with the above plot.  

As before, let's look at residuals.  

```{r}
uEWMA1.KFS <- KFS(uEWMA1fit$model)
uEWMA1.rstd <- rstandard(uEWMA1.KFS)
```

```{r,fig.cap = "Autocorrelation Function of Standardized Residuals"} 
(acf(uEWMA1.rstd, na.action=na.pass))
```

This suggest a great fit.  

What about plotting vs. time?  

```{r,fig.cap = "Standardized residuals from uEWMA1"}
ylim.std <- range(uEWMA1.rstd, na.rm=TRUE)
plotPresWars(uEWMA1.rstd, GDP, 
             main="residuals from EWMA fit", ylim=ylim.std, 
             ylab='residuals', las=1, 
             xlab='Year', yHoover=0.07, yFDR=0.25, 
             yWar=seq(.5, by=-.05, len=9))
```

As another check, let's make a normal probability plot of these residuals to look for outliers:  

```{r,fig.cap = "Normal probability plot of Standardized Residuals"} 
qqnorm(uEWMA1.rstd, datax=TRUE, 
       ylab='standardized residuals', 
       xlab='standard normal quantiles')
```

What happens if we delete all the observations prior to 1890?  

```{r,fig.cap = "Normal probability plot of Standardized Residuals"} 
goodUnempData <- with(GDP, 1889<Year)
qqnorm(uEWMA1.rstd[goodUnempData], datax=TRUE, 
       ylab='standardized residuals', 
       xlab='standard normal quantiles')
```

This doesn't look great.  What happens if we force the observation error variance to increase for older data?  

# 10.  Modeling unemployment with higher observation variance for older data 

For this, we can use the same [formula](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html) as for uEWMA1, but we'll need to make the observation error variane, H, time-varying, and we'll also need a special update function to support esimating parameters for the time-varying H:

<center> 
[formula](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html) -> [(time-varying observation error variance, H)](http://rpackages.ianhowson.com/cran/KFAS/man/KFAS.html) -> [SSModel](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) 

-> [(updatefn)](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) -> [fitSSM](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) -> [KFS](http://rpackages.ianhowson.com/cran/KFAS/man/KFS.html)  
</center>

```{r}
uEWMA1H <- array(NA, c(1, 1, N), 
  dimnames=list('probitUnemp', 'probitUnemp', GDP$Year))

uEWMA1aMdl. <-SSModel(uEWMA1fmla, GDP, H=uEWMA1H)
uEWMA1aMdl <- fixModelNames(uEWMA1aMdl., 'level', 
                    'probitUnemp')

uEWMA1aInit <- rep(0, 5)
names(uEWMA1aInit) <- c('lnQ', paste0('lnH', 1:4))
uEWMA1aUpdt <- function(pars=uEWMA1aInit, model=uEWMA1aMdl, 
                           ...){
  vars <- exp(pars)
  Q <- model$Q
  Q[1,1,] <- vars[1]
  model$Q <- Q
#
  Yrs <- time(uEWMA1mdl$y)
  H <- (vars[5]+(Yrs<1940)*vars[4] + 
          (Yrs<1931)*vars[3]+(Yrs<1890)*vars[2])
  model$H[] <- H
  model
} 

uEWMA1aFit <- fitSSM(uEWMA1aMdl, inits=uEWMA1aInit, 
                     updatefn=uEWMA1aUpdt)

logLiks[10, -1] <- logLik2(uEWMA1aFit)
logLiks[1:10, ]
```

What about the estimated variance components?  

```{r}
exp(uEWMA1fit$optim.out$par/2)
exp(uEWMA1aFit$optim.out$par/2)
```

Let's plot the estimated observation error standard deviations from this just to make sure we understand the corresponding parameter estimates:  

```{r,fig.cap = "Estimated observation error standard deviation"} 
uEWMA1aMdlEst <- uEWMA1aUpdt(uEWMA1aFit$optim.out$par)
str(uEWMA1aH. <- uEWMA1aMdlEst$H)
uEWMA1aH <- ts(as.numeric(uEWMA1aH.), 1790)
plotPresWars(sqrt(uEWMA1aH), GDP, yFDR=.5, 
             yWar=seq(.5, by=-.1, len=9), 
             xlab='', ylab='sd(observation error)')
```

This suggests a drop variance in 1931.  We would have expected a drop in 1890.  To confirm that, let's try constrained optimization:

```{r}
uEWMA1a1Init <- rep(1, 5)
names(uEWMA1a1Init) <- c('Q', paste0('H', 1:4))
uEWMA1a1Updt <- function(pars=uEWMA1a1Init, model=uEWMA1aMdl, 
                           ...){
  vars <- (pars^2)
  Q <- model$Q
  Q[1,1,] <- vars[1]
  model$Q <- Q
#
  Yrs <- time(uEWMA1mdl$y)
  H <- (vars[5]+(Yrs<1940)*vars[4] + 
          (Yrs<1931)*vars[3]+(Yrs<1890)*vars[2])
  model$H[] <- H
  model
} 

uEWMA1a1Fit <- fitSSM(uEWMA1aMdl, inits=uEWMA1a1Init, 
                     updatefn=uEWMA1a1Updt, 
                     method="L-BFGS-B", lower=0)
uEWMA1a1Fit$optim.out$par
logLiks[10, -1] <- logLik2(uEWMA1a1Fit)
logLiks[1:10, ]
logLik2(uEWMA1aFit)
```

The constrained optimization provides a better fit.  Let's plot the standard deviation of observation error for this better fitting model:  

```{r,fig.cap = "Estimated observation error sd for better fit"} 
uEWMA1a1MdlEst <- uEWMA1a1Updt(uEWMA1a1Fit$optim.out$par)
str(uEWMA1a1H. <- uEWMA1a1MdlEst$H)
uEWMA1a1H <- ts(as.numeric(uEWMA1a1H.), 1790)
plotPresWars(sqrt(uEWMA1a1H), GDP, yFDR=.5, 
    yWar=seq(.5, by=-.1, len=9), xlab='', 
    ylab='sd(Observation error)')
```

Let's test the additional 3 parameters:  

```{r}
uEWMA1chisq <- 2*apply(logLiks[9:10, 3:4], 2, diff)
pchisq(uEWMA1chisq, 3, lower=FALSE)
```

Just barely statistically signicant at the traditional 0.05 level: one chance in almost 30 of getting a result at least this extreme by chance alone.  

Did this help with the residuals?  

```{r,fig.cap = "Autocorrelation Function of Standardized Residuals"} 
uEWMA1a.KFS <- KFS(uEWMA1a1Fit$model)
uEWMA1a.rstd <- rstandard(uEWMA1a.KFS)

(acf(uEWMA1a.rstd, na.action=na.pass))
```

```{r,fig.cap = "Standardized residuals from uEWMA1"}
ylim.std <- range(uEWMA1a.rstd, na.rm=TRUE)
plotPresWars(uEWMA1a.rstd, GDP, 
             main="residuals from EWMA fit", ylim=ylim.std, 
             ylab='residuals', las=1, 
             xlab='Year', yHoover=0.07, yFDR=0.25, 
             yWar=seq(.5, by=-.05, len=9))

qqnorm(uEWMA1a.rstd[goodUnempData], datax=TRUE, 
       ylab='standardized residuals', 
       xlab='standard normal quantiles')
```

No obvious improvement over uEWMA1.  

Let's look for a presidential effect.  

# 11.  Modeling unemployment with a slope adjustment for each president

Do different presidents impact unemployment?  

One answer to this question uses a 2-dimensional state vector of level (of qnorm(unemployment)) and a slope term that restarts with each administration similar to the "pres3" term with EWMA2pres3.  For this we use a state transition matrix like the following:  

$$T_t = \left[\begin{array}
{rrr}
1 & 1 \\
0 & \gamma_t 
\end{array}\right],
$$ 

where $\gamma_t$ = 0 for the last year of a presidency, and 1 otherwise, as with EWMA2pres2 and EWMA2pres3.  

We combine this with migration variance $Q_t$ as follows:  

$$Q_t = \left[\begin{array}
{rrr}
q_{\mathrm{lvl}} & 0 \\
0 & q_{\mathrm{pres3}} (1-\gamma_t) \\
\end{array}\right], 
$$  

where $q_{\mathrm{lvl}}$ is the migration variance for the level, and $q_{\mathrm{pres3}}$ = $P_{1,2,2}$ is the variance of the presidential effect.  The rest of $P_1$ is 0, and $P_{1,\infty}$ is a diagonal matrix with elements (1, 0) indicating a diffuse prior for level but not the presidential effect, which we estimate.  

We will create this by modifying a double EWMA following a process similar to the above:  

<center> 
[(time-varying migration variance, Q)](http://rpackages.ianhowson.com/cran/KFAS/man/KFAS.html) -> [formula](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html) -> [(time-varying observation error variance, H)](http://rpackages.ianhowson.com/cran/KFAS/man/KFAS.html) -> [SSModel](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) 

--> [(time-varying state transition matrix, T)](http://rpackages.ianhowson.com/cran/KFAS/man/KFAS.html) -> [(updatefn)](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) -> [fitSSM](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) -> [KFS](http://rpackages.ianhowson.com/cran/KFAS/man/KFS.html)  
</center>

We can reuse EWMA2pres1Q.slope for Q.  We will modify the formula, similar to what we did with EWMA2pres1fmla, above, but for this 2-dimensional state    

```{r}
a1.2u <- c(level=qnorm(unemp1), pres3=0)
uPres3states <- names(a1.2u)

uEWMA1pres3P1 <- diag(c(0, NA))
uEWMA1pres3P1inf <- diag(c(1, 0))
uEWMA1pres3fmla <- qnorm(unemployment) ~ 
  SSMtrend(2, Q = list(level=matrix(NA),
                       pres3=EWMA2pres1Q.slope), 
      P1=uEWMA1pres3P1, P1inf=uEWMA1pres3P1inf, a1=a1.2u,
           ynames='probitUnemp') 
```

For the time-varying observation variance we can reuse uEWMA1H.  We next create an [SSModel](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html), as before:   

```{r}
uEWMA1pres3mdl. <- SSModel(uEWMA1pres3fmla, GDP, 
                           H=uEWMA1H)
uEWMA1pres3mdl <- fixModelNames(uEWMA1pres3mdl., 
                      uPres3states, 'probitUnemp')
```

Now we need to create the desired time-varying state transition matrix, T:  

```{r}
uPres3T <- array(uEWMA1pres3mdl$T, dim=c(2,2,N), 
        dimnames=list(uPres3states, uPres3states, GDP$Year) )
uPres3T[2,2, presLastYr] <- 0
uPres3T[,,1:9]

uEWMA1pres3mdl$T <- uPres3T 
```

As for EWMA2pres2mdl, we need also to fix the 'tv' attribute:  

```{r}
attr(uEWMA1pres3mdl, 'tv') <- c(Z=0L, H=1L, T=1L, R=0L, Q=1L)
```

We now have 6 parameters to estimate:  lnQ.lvl, lnQ.pres3, and lnH1..4.  We need starting values for the parameters to be estimated. As before, we select starting values based on the preceding fit:  

```{r}
uEWMA1aFit$optim.out$par
uEWMA1pres3init <- rep(uEWMA1aFit$optim.out$par, c(2, rep(1, 4)))
names(uEWMA1pres3init)[1:2] <- c("lnQ.lvl", "lnQ.pres3")
uEWMA1pres3init
```

This different model requires a different 'updatefn':   

```{r}
uEWMA1pres3updt <- function(pars=uEWMA1pres3init, 
                      model=uEWMA1pres3mdl, ...){
  vars <- exp(pars)
  Q <- model$Q
  Q[1,1,] <- vars[1]
  model$P1[2,2] <- vars[2]
  Q[2,2,presLastYr] <- vars[2]
  model$Q <- Q
#
  Yrs <- time(uEWMA1mdl$y)
  H <- (vars[6]+(Yrs<1940)*vars[5] + 
          (Yrs<1931)*vars[4]+(Yrs<1890)*vars[3])
  model$H[] <- H
  model
}

uEWMA1pres3fit <- fitSSM(uEWMA1pres3mdl,
    inits=uEWMA1pres3init, updatefn=uEWMA1pres3updt) 
logLiks[11, -1] <- logLik2(uEWMA1pres3fit)
logLiks[1:11,]
(lrchisq11 <- 2*apply(logLiks[10:11, 3:4], 2, diff))
pchisq(lrchisq11, 1, lower=FALSE)
```

The increase in the log(likelihood) is just over 0.8, so the likelihood ratio chi-square is just over 1.6.  That's not statistically signficant, as confirmed by the p-value of the reference chi-square distribution.  

Let's confirm this using constrained optimization:  

```{r}
uEWMA1pres3Cinit <- exp(uEWMA1pres3fit$optim.out$par/2)
names(uEWMA1pres3Cinit) <- sub('^ln', 's', 
                    names(uEWMA1pres3init))
uEWMA1pres3Cinit
uEWMA1pres3Cupdt <- function(pars=uEWMA1pres3Cinit, 
                      model=uEWMA1pres3mdl, ...){
  vars <- (pars^2)
  Q <- model$Q
  Q[1,1,] <- vars[1]
  model$P1[2,2] <- vars[2]
  Q[2,2,presLastYr] <- vars[2]
  model$Q <- Q
#
  Yrs <- time(uEWMA1mdl$y)
  H <- (vars[6]+(Yrs<1940)*vars[5] + 
          (Yrs<1931)*vars[4]+(Yrs<1890)*vars[3])
  model$H[] <- H
  model
}

uEWMA1pres3Cfit <- fitSSM(uEWMA1pres3mdl,
    inits=uEWMA1pres3Cinit, updatefn=uEWMA1pres3Cupdt, 
    method="L-BFGS-B", lower=0) 
uEWMA1pres3Cfit$optim.out$par
logLik2(uEWMA1pres3Cfit)
```

There's a slight change in the values of the estimated parameters. However, there was no detectable change in the log(likelihoods) or in rmse.pbtUnemp.  

What about a war effect?  

# 12. Modeling unemployment with a fixed slope adjustment for war

The plots above suggest that war reduces unemployment.  Let's look for a fixed effect with a 3-dimensional state vector and a transition matrix as when we looked for fixed effect of war on income:  

$$T_t = \left[\begin{array}
{rrrr}
1 & w_0 & w_1 \\
0 & 1 & 0 \\
0 & 0 & 1 
\end{array}\right],
$$ 

where $w_0$ and $w_1$ indicate the presence of war and battle deaths per million population, as before.

For this model, Q is constant, and we want a diffuse prior on all 3 components of the initial state vector.  Thus, $P_{1,\infty}$ is the 3-dimensional identity matrix, and $P_1$ is the 3-dimensional zero matrix.  

For this model, we need an update function to make H vary with time like we want.  

<center> 
[formula](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html) -> [(time-varying observation error variance, H)](http://rpackages.ianhowson.com/cran/KFAS/man/KFAS.html) -> [SSModel](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) --> 

[(time-varying state transition matrix, T)](http://rpackages.ianhowson.com/cran/KFAS/man/KFAS.html) -> [(updatefn)](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) -> [fitSSM](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) -> [KFS](http://rpackages.ianhowson.com/cran/KFAS/man/KFS.html)  
</center>

As before, we'll let $R_t$ = the 3-dimensional identity matrix with $Q_t$ being a diagonal matrix with only the first element nonzero.

We'll use a similar formula to those above, requesting a third degree [SSMtrend](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) and modifying $T_t$ as before:  

```{r}
a1.3u <- c(level=qnorm(unemp1), war=0, deaths=0)
uWarStates <- names(a1.3u)

uEWMA1war0fmla <- qnorm(unemployment) ~ 
    SSMtrend(3, Q = list(level=matrix(NA), 
                war=matrix(0), deaths=matrix(0)), a1=a1.3u,
           ynames='probitUnemp') 

uEWMA1war0mdl. <- SSModel(uEWMA1war0fmla, GDP, H=uEWMA1H)
uEWMA1war0mdl <- fixModelNames(uEWMA1war0mdl., uWarStates, 
                               'probitUnemp')
```

Now we need to create the desired time-varying state transition matrix, $T_t$:  

```{r}
uWar0T <- array(diag(3), dim=c(3,3,N), 
    dimnames=list(uWarStates, uWarStates, GDP$Year))
uWar0T[1, 2,] <- (GDP$war!='')
uWar0T[1, 3,] <- GDP$battleDeathsPMP
uWar0T[,,1:9]
```

Let's check this $T_t$ with years at war and peace.  

```{r}
GDP[c(1, 2, 7), c('Year', 'war', 'battleDeathsPMP')]
uWar0T[,,c(1, 2, 7)]
```

They match.  Let's save this T as part of uEWMA1war0mdl:  

```{r}
uEWMA1war0mdl$T <- uWar0T 
```

We also need to fix the 'tv' attribute, as we did for models 4 and 5 (EWMA2pres2 and EWMA2pres3):  

```{r}
attr(uEWMA1war0mdl, 'tv') <- c(Z=0L, H=1L, T=1L, R=0L, Q=0L)
```

For starting values, let's look at the the parameter estimates for uEWMA1a:  

```{r}
uEWMA1aFit$optim.out$par
```

We'll use the first one but the second and fourth look too small for a starting value for the logarithm of a variance.  Let's replace lnH1..4 by lnH3:  

```{r}
uEWMA1war0init <- rep(uEWMA1aFit$optim.out$par[c(1, 3)], c(1, 4))
names(uEWMA1war0init) <- names(uEWMA1aFit$optim.out$par)
uEWMA1war0init
```

We need an update function to modify Q and H like we want:  

```{r}
uEWMA1war0updt <- function(pars=uEWMA1war0init, 
                      model=uEWMA1war0mdl, ...){
  vars <- exp(pars)
  Q <- model$Q
  Q[1,1,] <- vars[1]
  model$Q <- Q
#
  Yrs <- time(uEWMA1mdl$y)
  H <- (vars[5]+(Yrs<1940)*vars[4] + 
          (Yrs<1931)*vars[3]+(Yrs<1890)*vars[2])
  model$H[] <- H
  model
}

uEWMA1war0fit <- fitSSM(uEWMA1war0mdl, inits=uEWMA1war0init, 
                        uEWMA1war0updt)
logLik2(uEWMA1war0fit)
```

The diffuse likelihood declined from uEWMA1a but the marginal likelihood increased and rmse.pbtUnemp also increased.  This is plausible, but let's revise this to use constrained estimation:


```{r}
uEWMA1war0Cinit <- exp(uEWMA1war0fit$optim.out$par/2)
names(uEWMA1war0Cinit) <- sub('ln', 's', 
                              names(uEWMA1war0Cinit))
uEWMA1war0Cinit
uEWMA1war0Cupdt <- function(pars=uEWMA1war0Cinit, 
                      model=uEWMA1war0mdl, ...){
  vars <- (pars^2)
  Q <- model$Q
  Q[1,1,] <- vars[1]
  model$Q <- Q
#
  Yrs <- time(uEWMA1mdl$y)
  H <- (vars[5]+(Yrs<1940)*vars[4] + 
          (Yrs<1931)*vars[3]+(Yrs<1890)*vars[2])
  model$H[] <- H
  model
}

uEWMA1war0Cfit <- fitSSM(uEWMA1war0mdl, inits=uEWMA1war0Cinit, 
          uEWMA1war0Cupdt, method="L-BFGS-B", lower=0)
logLiks[12, -1] <- logLik2(uEWMA1war0Cfit)
logLiks[1:12,]
(uEWMA1war0Chisq <- apply(logLiks[c(10, 12), 2:4], 2, diff))
pchisq(2*uEWMA1war0Chisq[3], uEWMA1war0Chisq[1], lower=FALSE)
```

Statistically signficant at the traditional 0.01 level. This seems to be a better model, even if the unweighted rmse.pbtUnemp is larger.  

Let's try a random slope similar to EWMA2war1:    

# 14. Modeling unemployment with a random slope adjustment for war

This model converts the fixed-effect model of uEWMA1war0 to a random effect, similar to the presidential effect in EWMA2pres3, except that the war effect is 2-dimensional, while the presidential effect occupied only one component of the state vector.  

In paricular we will use here a time-varying state transition matrix as follows:

$$T_t = \left[\begin{array}
{rrrr}
1 & w_0 & w_1 \\
0 & w_0 & 0 \\
0 & 0 & w_0 
\end{array}\right],
$$ 

where $w_0$ and $w_1$ indicate the presence of war and battle deaths per million population, as before.  And we need to make the migration variance time-varying to match: 

<center> 
[(time-varying migration variance, Q)](http://rpackages.ianhowson.com/cran/KFAS/man/KFAS.html) -> [formula](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html) -> [(time-varying observation error variance, H)](http://rpackages.ianhowson.com/cran/KFAS/man/KFAS.html) ->
[SSModel](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) 

-> [(time-varying state transition matrix, T)](http://rpackages.ianhowson.com/cran/KFAS/man/KFAS.html) -> [(updatefn)](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) -> [fitSSM](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) -> [KFS](http://rpackages.ianhowson.com/cran/KFAS/man/KFS.html)  
</center>

Similar to EWMA2war1 , we will allow Q for the (war, deaths) components to be nonzero the year before the start of each war:  Specifically, we want Q[1, 1, ] to be NA, Q[2:3, 2:3, beforeWar] to NA;  all other elements of Q should be 0:  

```{r}
uEWMA1warQ <- array(0, c(3,3,N), 
    dimnames=list(uWarStates, uWarStates, GDP$Year))
uEWMA1warQ[1,1,] <- NA
uEWMA1warQ[2:3,2:3,beforeWar] <- NA 

uEWMA1warQ[,,beforeWar[1]+(-1):1]
```

That looks correct.  

[SSMtrend(3, ...)](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) requires Q to be a list of length 3, so we can't use that. Instead, we'll copy the previous [SSModel](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) and modify Q directly, as we did for EWMA2war1:  

```{r}
uEWMA1war1mdl <- uEWMA1war0mdl
uEWMA1war1mdl$Q <- uEWMA1warQ
attr(uEWMA1war1mdl, 'tv')[5] <- 1L
```

Since we are estimating P1[2:3, 2:3] = Q[2:3, 2:3, beforeWar], we need to set P1inf to diag(c(1,0,0)):  

```{r}
uEWMA1war1mdl$P1inf <- diag(c(1,0,0))
```

We also want to set T[3:4, 3:4, GDP$war==''] to 0:    

```{r}
uWar1T <- uWar0T
uWar1T[2:3, 2:3, GDP$war==''] <- 0
uWar1T[,,c(1:2, 8, 22:24)]
```

Looks good.  Let's put it in the model:  

```{r}
uEWMA1war1mdl$T <- uWar1T
```

We also need a matching updatefn.  That requires specifying the parameters to be estimated, for which we will ultimately need starting values.  What were the paramter estimates for the three models most like the current one?  

```{r}
uEWMA1aFit$optim.out$par
uEWMA1pres3fit$optim.out$par
uEWMA1war0fit$optim.out$par
```

For a random effect of war, we'll want to include 3 parameters for the [nlme::pdLogChol](http://rpackages.ianhowson.com/cran/nlme/man/pdLogChol.html) representation of Q[3:4, 3:4].  We'll call these parameters lnQ.war, lnQ.deaths, and lnQ.warDeaths.  However, since the first two of these parameters are more like standard deviations than variances, we'll adjust their starting values accordingly.  And the lnQ.warDeaths is more like a regression coefficient, and we'll start that at 0:   

```{r}
uEWMA1war1init <- c(lnQ=-2.66, lnQ.war=-1,
    lnQ.deaths=-1, lnQ.warDeaths=0, lnH1=-4, 
    lnH2=-4, lnH3=-4, lnH4=-4)
uEWMA1war1init

uEWMA1war1updt <- function(pars=uEWMA1war1init, 
                      model=uEWMA1war1mdl, ...){
  Q <- model$Q
  Q[1,1,] <- exp(pars[1])
#  
  P1w <- as.matrix(nlme::pdLogChol(pars[2:4]))
  model$P1[2:3, 2:3] <- P1w
  Q[2:3, 2:3, beforeWar] <- P1w
  model$Q <- Q
# 
  Yrs <- time(uEWMA1mdl$y)
  H <- (exp(pars[8])+(Yrs<1890)*exp(pars[7]) +
      (Yrs<1931)*exp(pars[6])+(Yrs<1890)*exp(pars[5]))
  model$H[] <- H 
  model
} 

uEWMA1war1fit <- fitSSM(uEWMA1war1mdl,
        inits=uEWMA1war1init, 
        updatefn=uEWMA1war1updt)
logLik2(uEWMA1war1fit)
```

This shows two major problms:  

  1.  "Warning ... Negative variances in V".  
  1.  The log(likelihoods) are not even close to the log(likelihoods) for the other unemployment models, and the root mean square one-step ahead prediction errors is almost triple that for the other unemployment models.  
  
Spot checks of the model failed to identify a problem. Let's try constrained optimization:  

```{r}
uEWMA1war1Cinit <- exp(uEWMA1war1init)
names(uEWMA1war1Cinit) <- sub('ln', 's', 
                  names(uEWMA1war1init))
uEWMA1war1Cinit[4] <- 0 
names(uEWMA1war1Cinit)[4] <- 'rQ.warDeaths' 
uEWMA1war1Cinit

uEWMA1war1Cupdt <- function(pars=uEWMA1war1Cinit, 
                      model=uEWMA1war1mdl, ...){
  vars <- pars^2
  Q <- model$Q
  Q[1,1,] <- vars[1]
#  
  C1w <- matrix(rep(pars[4], 4), 2,2) 
  diag(C1w) <- 1
  P1w <- (C1w * outer(pars[2:3], pars[2:3]))
  model$P1[2:3, 2:3] <- P1w
  Q[2:3, 2:3, beforeWar] <- P1w
  model$Q <- Q
# 
  Yrs <- time(uEWMA1mdl$y)
  H <- (vars[8]+(Yrs<1890)*vars[7] +
      (Yrs<1931)*vars[6]+(Yrs<1890)*vars[5])
  model$H[] <- H 
  model
} 

uEWMA1war1l <- uEWMA1war1Cinit
uEWMA1war1l[] <- 0
uEWMA1war1l[4] <- (-1)
uEWMA1war1u <- uEWMA1war1Cinit
uEWMA1war1u[] <- Inf
uEWMA1war1u[4] <- 1

uEWMA1war1Cfit <- fitSSM(uEWMA1war1mdl,
    inits=uEWMA1war1Cinit, updatefn=uEWMA1war1Cupdt,
    method="L-BFGS-B", 
    lower=uEWMA1war1l, upper=uEWMA1war1u)
logLiks[13, -1] <- logLik2(uEWMA1war1Cfit)
logLiks[1:13,]
```

The technical problem disappeared:  No "Negative variances in V," the log(likelihoods) match those from uEWMA1a, and the rmse differs in the 6th significant digit.  

Regarding the impact of war on unemployment, this model suggests none.  

Let's look for a variance effect of war, similar to EWMA2war2:  

# 15.  Modeling unemployment with a variance effect for war 

Here we allow for substantial, single-year changes in the level in uEWMA1a by modeling log(Q[1,1]) ~ war + deaths.  

This model has a 3-dimensional state vector:  the level and the random adjustments for 




For this, we start with uEWMA1a and make Q time-varying with an appropriate updatefn.  

```{r}
uEWMA1war2mdl <- uEWMA1aMdl
uEWMA1war2Q <- array(uEWMA1war2mdl$Q, dim=c(1,1,N), 
    dimnames=list('level', 'level', GDP$Year))
uEWMA1war2Q[,, 1:2, drop=FALSE]
```

This looks OK.  

```{r}
uEWMA1war2mdl$Q <- uEWMA1war2Q
attr(uEWMA1war2mdl, 'tv') <- 
        c(Z=0L, H=0L, T=0L, R=0L, Q=1L)
```

We need starting values for the numerical optimizer.  Let's look at the optimal values found for uEWMA1a:  

```{r}
uEWMA1aFit$optim.out$par

uEWMA1war2init <- with(uEWMA1aFit$optim.out, 
      c(par[1], 0, 0, rep(par[3], 4)) )
names(uEWMA1war2init) <- c('lnQ.lvl', 
    'lnQ.lvl.war','lnQ.lvl.deaths', 
    paste0('lnH', 1:4))
uEWMA1war2init
```

This looks like plausible starting values.  

 ``{r}
uEWMA1war2updt <- function(pars=uEWMA1war2init, 
                      model=uEWMA1war2mdl, ...){
#
  Q <- model$Q
  lnQ11 <- (pars[1] + pars[2]*(GDP$war!='') +
                   pars[3]*GDP$battleDeathsPMP)
  Q11 <- exp(lnQ11)
  oops <- which(!is.finite(Q11))
  Q11[oops] <- .Machine$double.xmax
#  
#  noops <- length(oops)
#  if(noops>0){
#    cat('Q[1,1] NA or Inf with pars =\n')
#    print(pars)
#    stop('Problem occurs ', noops, ' times, ', 
#         'starting with number ', noops[1], 
#         ' for which Q[1,1] = ', Q11[oops[1]])
#  }
  Q[1,1,] <- Q11
#
  model$Q <- Q
#  
  vars <- exp(pars)
  Yrs <- time(uEWMA1mdl$y)
  H <- (vars[7]+(Yrs<1890)*vars[6] +
      (Yrs<1931)*vars[5]+(Yrs<1890)*vars[4])
  model$H[] <- H 
#    
  model
} 
uEWMA1war2updt()

## ** JOUNI:  
## I might modify the following 
## to complete this model ... 
## if I decide to complete it ... 

EWMA2war2fit <- fitSSM(EWMA2war2mdl, inits=EWMA2war2init, 
                        updatefn=EWMA2war2updt)
logLiks[8, -1] <- logLik2(EWMA2war1fit)
logLiks[1:8,1:5]
```

This matches the initial log-scale fit for EWMA2war1.  We'd like to try a constrained optimization over standard deviations for this, as with EWMA2war1 above.  However, it's less that obvious how to transform the war effect.  For this, we'll use the following:  

  Q[1,1] = (sQ.lvl^2)*exp(lnQ.lvl.war*war 
                  + lnQ.lvl.deaths*deaths)

```{r}
EWMA2war2init2 <- exp(EWMA2war2init/2)
EWMA2war2init2[2:3] <- 0 
names(EWMA2war2init2) <- c('sQ.lvl', 'lnQ.lvl.war',
                'lnQ.lvl.deaths', 'sQ.slope', 'sH')
EWMA2war2init2

EWMA2war2updt2 <- function(pars=EWMA2war2init2, 
                      model=EWMA2war2mdl, ...){
#
  Q <- model$Q
  Q11 <- ((pars[1]^2)*exp(pars[2]*(GDP$war!='') +
                   pars[3]*GDP$battleDeathsPMP))
  oops <- which(!is.finite(Q11))
  Q11[oops] <- .Machine$double.xmax
#  
#  noops <- length(oops)
#  if(noops>0){
#    cat('Q[1,1] NA or Inf with pars =\n')
#    print(pars)
#    stop('Problem occurs ', noops, ' times, ', 
#         'starting with number ', noops[1], 
#         ' for which Q[1,1] = ', Q11[oops[1]])
#  }
  Q[1,1,] <- Q11
#
  Q22 <- pars[4]^2
  if(!is.finite(Q22)){
    print(pars)
    stop('Q22 = ', Q22)
  }
  
  Q[2,2,] <- Q22
  model$Q <- Q
#  
  model$H[] <- (pars[5]^2)
  model
} 
EWMA2war2updt2()

EWMA2war2fit2 <- fitSSM(EWMA2war2mdl, inits=EWMA2war2init2, 
                        updatefn=EWMA2war2updt2)
logLiks[8, -1] <- logLik2(EWMA2war2fit2)
logLiks[1:8,1:5]
```

This is the best fitting war model so far with a modest increase in the marginal log(likelihood) over EWMA2:  

```{r}
(d2.8 <- apply(logLiks[c(2, 8), 2:5], 2, diff))
pchisq(2*d2.8[3], d2.8[1], lower=FALSE)
```








We could try a model with a variance effect for war.  However, when we tried that with average annual income (EWMA2war2), we got nothing useful.  Moreover uEWMA2war0 seemed like a pretty good model.  

We did  not find a president effect on unemployment when we looked for it by itself.  However, since uEWMA1war0 fit so well, we might find a president effect if combined with war.  

# 15.  Modeling unemployment with slope adjustments for presidents and war 

To add a president effect to uEWMA1war0, we will combine the models for EWMA2pres3 and uEWMAwar0.  We'll use a state transition matrix as follows:  

$$T_t = \left[\begin{array}
{rrrr}
1 & 1 & w_0 & w_1 \\
0 & \gamma_t & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 
\end{array}\right],
$$ 

where $\gamma_t$ = 0 for the last year of a presidency, and 1 otherwise, as with EWMA2pres2 and EWMA2pres3.  

We combine this with migration variance $Q_t$ as follows:  

$$Q_t = \left[\begin{array}
{rrrr}
q_{\mathrm{lvl}} & 0 & 0 & 0 \\
0 & q_{\mathrm{pres3}} (1-\gamma_t) & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 
\end{array}\right],
$$ 

where $q_{\mathrm{lvl}}$ is the migration variance for the level, $q_{\mathrm{pres3}}$ = $P_{1,2,2}$ is the variance of the presidential effect, and $Q_{\mathrm{war}}$ is the 2 x 2 covariance matrix of the war effect.  

To build this, we'll need all the steps we've used before to add "pres3" to the state vector, construct time-varying matrices for state transition, T, transition variance, Q, and observation variance, H:  

<center> 
[(time-varying migration variance, Q)](http://rpackages.ianhowson.com/cran/KFAS/man/KFAS.html) -> [formula](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html) -> [(time-varying observation error variance, H)](http://rpackages.ianhowson.com/cran/KFAS/man/KFAS.html) ->

[SSModel](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) -> [(time-varying state transition matrix, T)](http://rpackages.ianhowson.com/cran/KFAS/man/KFAS.html) -> [(updatefn)](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) -> [fitSSM](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) -> [KFS](http://rpackages.ianhowson.com/cran/KFAS/man/KFS.html)  
</center>

As before, we'll let $R_t$ = the 4-dimensional identity matrix.  We create Q as follows:  

```{r}
uP3w0states <- c('level', 'pres3', 'war', 'deaths')
uP3w0Q <- array(0, c(4,4,N), 
    dimnames=list(uP3w0states, uP3w0states, GDP$Year))
uP3w0Q[1,1,] <- NA
uP3w0Q[2, 2, presLastYr] <- NA

uP3w0Q[,,beforeWar[1]+(-3):3]
```

That looks correct.  Rather than use SSMtrend(4, ...), we'll just modify EWMA2war1mdl:  

```{r}
a1.4u <- c(level=qnorm(unemp1), pres3=0, war=0, deaths=0)
uP3w0States <- names(a1.4u)

uEWMA1p3w0mdl <- fixModelNames(EWMA2war1mdl, uP3w0States, 
                               'probitUnemp')
uEWMA1p3w0mdl$y <- uEWMA1mdl$y
uEWMA1p3w0mdl$H <- uEWMA1H
uEWMA1p3w0mdl$Q <- uP3w0Q
```

Next, we create the desired time-varying state transition matrix, $T_t$:  

```{r}
uP3w0T <- array(diag(4), dim=c(4,4,N), 
    dimnames=list(uP3w0States, uP3w0States, GDP$Year))
uP3w0T[1, 2,] <- 1
uP3w0T[2,2, presLastYr] <- 0 
uP3w0T[1, 3,] <- (GDP$war!='')
uP3w0T[1, 4,] <- GDP$battleDeathsPMP
uP3w0T[,,1:9]
```

Let's check this $T_t$ with years at war and peace.  

```{r}
GDP[c(1, 2, 7), c('Year', 'war', 'battleDeathsPMP')]
uP3w0T[,,c(1, 2, 7)]
```

They match.  Let's save this T as part of uEWMA1p3w0mdl:  

```{r}
uEWMA1p3w0mdl$T <- uP3w0T 
```

We also need to fix the 'tv' attribute:  

```{r}
attr(uEWMA1p3w0mdl, 'tv') <- c(Z=0L, H=1L, T=1L, R=0L, Q=1L)
```

For starting values, let's look at the the parameter estimates for uEWMA1war0fit:  

```{r}
uEWMA1war0fit$optim.out$par

uEWMA1p3w0init <- c(lnQ.lvl=-2.66, lnQ.pres3=-2, 
    lnH1=-4, lnH2=-8)

uEWMA1p3w0updt <- function(pars=uEWMA1p3w0init, 
                      model=uEWMA1p3w0mdl, ...){
  vars <- exp(pars)
  Q <- model$Q
  Q[1,1,] <- vars[1]
  Q[2,2, presLastYr] <- vars[2]
  model$Q <- Q
# 
  Yrs <- time(model$y)
  H <- (vars['lnH1']+(Yrs<1890)*vars['lnH2'])
  model$H[] <- H 
  model
}

uEWMA1p3w0fit <- fitSSM(uEWMA1p3w0mdl, inits=uEWMA1p3w0init, 
                        uEWMA1p3w0updt)
logLiks[15, -1] <- logLik2(uEWMA1p3w0fit)
logLiks[13:15, ] 
```

This model has major problems:  Most importantly, we get, "Warning ... Either model is degenerate or numerical errors occured."  This warning was generated by the call from logLik2 to [KFS](http://rpackages.ianhowson.com/cran/KFAS/man/KFS.html):  

```{r}
(uEWMA1p3w0KFS <- KFS(uEWMA1p3w0fit$model))$d
```

Per the [KFS](http://rpackages.ianhowson.com/cran/KFAS/man/KFS.html) documentation, "d" is the length of the diffuse phase.  Here, "d" = 226, the length of the series.  

Quite likely, the "pres3" and "war0" effects are [confounded](https://en.wikipedia.org/wiki/Confounding) and therefore not independently estimable.  We won't take the time to check that here.  

Instead, we will proceed with modeling log(realGDPperCapita) and qnorm(unemployment) with separate "level" parameters for both, looking for a correlation between the changes in the levels, suggested by [Okun's law](https://en.wikipedia.org/wiki/Okun's_law).  In so doing, we will consider using "pres3" with "GDP.lvl" and "war0" with "unemp.lvl", but not "pres3" with "unemp.lvl" nor "war0" with "GDP.lvl", consistent with the previous work modeling univariate responses.  

# 16.  Modeling GDP and unemployment with Okun's law

[Okun's law](https://en.wikipedia.org/wiki/Okun's_law) suggests that changes in unemployment and GDP are negatively correlated.  `r Citet(bib2, "BallOkun")` reported different regression coefficients between the two in different countries.  In the present work we decompose the regression coefficient into three terms:  [the standard deviations of changes in log(realGDPperCapita) and qnorm(unemployment) and their correlation](http://people.duke.edu/~rnau/mathreg.htm). Thus, the differences between countries in the regression coefficient could be due to differences in either this correlation or in how narrowly different countries manage GDP growth or unemployment.  

We will extend this to modeling both GDP and unemployment in terms of a state vector with the following components:  

Component | Description 
:----|:----------------
GDP.lvl | level of log(realGDPperCapita)
unemp.lvl | level of qnorm(unemployment).  
GDP.slope | slope of log(realGDPperCapita)

This work deviates from the traditional literature on Okun's law in a couple of ways.  First, we use transformations of GDP and unemployment:  GDP can theoretically never be negative, but log(realGDPperCapita) can in theory range from minus infinity to plus infinity.  Similarly, unemployment is a number between 0 and 1, but qnorm(unemployment) again can in theory cover the entire real line.  It's difficult to use linear models in the untransformed space because of these constraints of positivity and (0, 1), respectively.  Second, we also consider the rate of growth in log(realGDPperCapita) and how its changes correlate with those in qnorm(unemployment).  

We will look for two correlations between changes in these components:  The single correlation that seems most closely related to Okun's law is that beteen changes in GDP.lvl and unemp.lvl.  However, the correlation between changes in GDP.slope and unemp.lvl also seems relevant to the study of the relationship between unemployment and economic growth.  We will assume zero correlation between changes in GDP.lvl and GDP.slope, as is commonly done in similar situations.  

Estimating the transition variance, Q, for this state vector provides an interesting technical challenge:  Q must be non-negative definite with one of its off-diagonal elements zero.  Fortunately, the [nlme::pdLogChol](http://rpackages.ianhowson.com/cran/nlme/man/pdLogChol.html) function, used above for Q.war, can do this for us provided we arrange for the zero off-diagonal element to be in the [3, 3] position.  To see this, note that if C is upper triangular with nonzero elements only on the [superdiagonal](https://en.wikipedia.org/wiki/Diagonal), then [crossprod(C)](https://stat.ethz.ch/R-manual/R-devel/library/base/html/crossprod.html) is [tridiagonal](https://en.wikipedia.org/wiki/Tridiagonal_matrix).  

Thus, by placing unemp.lvl between GDP.lvl and GDP.slope in the state vector, we can use pdLogChol to estimate a covariance matric with a zero for the corelation between the changes in GDP.lvl and GDP.slope.  We will call the five parameters we estimate for this lnQ.pl, lnQ.ul, lnQ.ps, lnQ.pl.ul, and lnQ.ul.ps.    

We could estimate this model with KFAS with an entirely univariate response updating alternately between log(realGDPperCapita) and qnorm(unemployment) -- or even mixing annual GDP figures with quarterly unemployment numbers or mixing annual and quarterly data in other ways. 

In fact, [fitSSM](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) converts a multivariate response to univariate.  We will not do that manually here, because it seems more convenient to use the multivariate response directly.   

In coding this we will follow the example on pp. 15ff of `r Citet(bib2, "KFASarticle")`, which also has a multi-dimensional response:  

<center> 
Response matrix -> Z, T, Q, a1, P1, P1inf -> [SSMcustom](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) in 
[formula](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html) 

-> H -> [SSModel](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) -> [(updatefn)](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) -> [fitSSM](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) -> [KFS](http://rpackages.ianhowson.com/cran/KFAS/man/KFS.html)  
</center>

```{r}
OkunDat <- cbind(lnGDPperCap=log(GDP$realGDPperCapita), 
           probitUnemp=qnorm(GDP$unemployment))
(OkunVars <- colnames(OkunDat))

(OkunStates <- c('GDP.lvl', 'unemp.lvl', 'GDP.slope'))
(OkunZ <- array(c(1, 0, 0, 1, 0, 0), dim=c(2, 3, 1), 
    dimnames=list(OkunVars, OkunStates, NULL) ) )
              
(OkunT <- array(c(1, 0, 0, 0, 1, 0, 1, 0, 1), dim=c(3, 3, 1), 
    dimnames=list(OkunStates, OkunStates, NULL) ) )

(OkunQ <- array(c(NA, NA, 0, NA, NA, NA, 0, NA, NA), 
    dim=c(3, 3, 1), 
    dimnames=list(OkunStates, OkunStates, NULL)))

Okun.a1 <- c(apply(OkunDat, 2, function(x)x[!is.na(x)][1]), 
             0) 
names(Okun.a1) <- OkunStates
Okun.a1

OkunP1 <- diag(0, 3)
dimnames(OkunP1) <- list(OkunStates, OkunStates)
OkunP1
OkunP1inf <- OkunP1
diag(OkunP1inf) <- 1
OkunP1inf

OkunFmla <- formula(OkunDat ~ -1 + SSMcustom(
  Z=OkunZ, T=OkunT, Q=OkunQ, a1=Okun.a1, 
  P1=OkunP1, P1inf=OkunP1inf, index=1:2, n=N,
  state_names=OkunStates) )

OkunH <- array(diag(NA, 2), dim=c(2, 2, N), 
    dimnames=list(OkunVars, OkunVars, GDP$Year) )

OkunMdl. <- SSModel(OkunFmla, H=OkunH)
OkunMdl <- fixModelNames(OkunMdl., OkunStates, 
                  OkunVars)
```

The Z, T, Q, a1, P1, and P1inf arrays created here all look plausible.  

For starting values, let's use EWMA2 and uEWMA1a, because the current model combines those two models allowing for correlations in the migration variance:  

```{r}
EWMA2fit$optim.out$par
uEWMA1aFit$optim.out$par

OkunInit <- rep(0, 8)
names(OkunInit) <- c("lnQ.pl", "lnQ.ul", "lnQ.ps", 
  "lnQ.pl.ul", "lnQ.ul.ps", "lnHp", "lnHu1", "lnHu2")
OkunInit[c(1, 3, 6)] <- EWMA2fit$optim.out$par
OkunInit[c(2, 7:8)] <- uEWMA1aFit$optim.out$par
OkunInit

pdLogChol3.5 <- function(value, ...){
  v6 <- c(value[1:4], 0, value[5])
  nlme::pdLogChol(v6)
}
(pdLCtst <- pdLogChol3.5(1:5))
as.numeric(pdLCtst)

OkunUpdt <- function(pars=OkunInit, model=OkunMdl, ...){
#  
  Q <- pdLogChol3.5(pars) 
  model$Q[,,] <- as.matrix(Q)
# 
  Yrs <- time(model$y)
  H <- model$H
  H[1,1,] <- exp(pars[6])
  Hu <- (exp(pars[7])+(Yrs<1890)*exp(pars[8]))
  H[2,2,] <- Hu
  model$H[] <- H 
  model
}
str(tst <- OkunUpdt())
as.numeric(nlme::pdLogChol(tst$Q[,,1]))
logLik(tst)
logLik(tst, marginal=TRUE)

OkunFit <- fitSSM(OkunMdl, OkunInit, OkunUpdt)
logLiks[16, -1] <- logLik2(OkunFit)
logLiks[1:16, ]
```

Unfortunately, "Computation of marginal likelihood failed", and the diffuse likelihood for this case cannot be compared with any of the previous models.  The rmse.lnUSD was better than EWMA1 but slightly worse than EWMA2:  0.0429 vs. 0.0456 and 0.0427.  However, the rmse.pbtUnemp was substantially lower than the best previous model:  0.268 vs. 0.283 for uEWMA1a and 0.284 for uEWMA1war0.  

What are the paramter estimates and the resulting correlations in Q?  

```{r}
OkunFit$optim.out$par
(OkunQ1.3 <- pdLogChol3.5(OkunFit$optim.out$par))
cov2cor(as.matrix(OkunQ1.3))
```

Wow:  The correlation between the increments in GDP.lvl and unemp.lvl is -0.999997.  The literature on Okun's Law says that the increments in GDP and unemployment are often cointegrated, e.g.,  `r Citet(bib2, "Lee2000")`, and negatively correlated.  Moreover the correlation between the increments in GDP.lvl and GDP.slope is estimated at -0.002 -- virtually zero.  

This suggests we modify this model to estimate only 6 parameters, not 8, and force those 2 correlations to -1 and 0, respectively.  We'll call this a separate model:  Okun1.  

# 17.  Okun's Law fixing migration correlations 

For this we only need to change the update function and starting values from the previous model:  

```{r}
EWMA2fit$optim.out$par
uEWMA1aFit$optim.out$par

(Okun1Init <- OkunInit[-(4:5)])

Okun1Updt <- function(pars=Okun1Init, model=OkunMdl, ...){
#  
  Q <- diag(exp(pars[1:3]))
  model$Q[] <- Q
#
  Yrs <- time(model$y)
  H <- model$H
  H[1,1,] <- exp(pars[4])
  Hu <- (exp(pars[5])+(Yrs<1890)*exp(pars[6]))
  H[2,2,] <- Hu
  model$H[] <- H 
  model
}
str(tst <- Okun1Updt())

Okun1Fit <- fitSSM(OkunMdl, Okun1Init, Okun1Updt)
logLiks[17, -1] <- logLik2(Okun1Fit)  
logLiks[1:17, ]
```  
  
Again, "Computation of marginal likelihood failed".  The diffuse likelihood got substantially worse, but it's not clear if we can interpret that in any meaningful way.  However, both of the root mean square error numbers, rmse.lnUSD and   rmse.pbtUnemp, got a little better.  

Let's manually cast this as univariate to see if the algorithm works better for the problem in that form.  

# 18.  Okun's Law as a univariate problem  

As previously mentioned, an alternative approach to this problen involves converting the bivariate response to a vector with double the length, then deleting the missing values. The primary change required do that is to use the first row of the previous Z when log(realGDPperCapita) is the response and the second row when qnorm(unemployment) is.  This needs to be matched by setting Q to 0 when moving from one response to the other for the same time (year, in this case).  

For this model, Z, Q and H must vary with time but not T.  We will proceed as before:  

<center> 
Response vector -> Z, T, Q, a1, P1, P1inf -> [SSMcustom](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) in 
[formula](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html) 

-> H -> [SSModel](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) -> [(updatefn)](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) -> [fitSSM](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) -> [KFS](http://rpackages.ianhowson.com/cran/KFAS/man/KFS.html)  
</center>

First we need cast OkunDat as a vector:  

```{r}
OkunDat1. <- matrix(as.numeric(t(OkunDat)), ncol=1)
OkunDat[1:2, ]
OkunDat1.[1:4, ]
```

That looks correct.  We need Year to match:  

```{r}
Year1. <- rep(GDP$Year, each=2)
Year1.[1:4]
```

Now let's delete the NAs:  

```{r}
OkunDel <- is.na(OkunDat1.)
OkunDat1 <- OkunDat1.[!OkunDel, , drop=FALSE]
Year1 <- Year1.[!OkunDel]
OkunDat[1:3,]
OkunDat1[1:3,]
Year1[1:3]
```

Let's write a function to recast OkunDat1 as OkunDat:  

```{r}
sameYr <- c(FALSE, 
    (head(Year1, -1)==tail(Year1, -1)))
OkunVec2mat <- function(x=OkunDat1){
  X <- matrix(NA, N, 2, dimnames=dimnames(OkunDat))
  X[, 1] <- x[!sameYr]
  X[GDP$Year>1799, 2] <- x[sameYr]
  X. <- ts(X, 1790)
  X.
}
OkunD <- OkunVec2mat()
all.equal(OkunDat, OkunD)
```

Good.  Now let's create Z to match:  

```{r}
OkunZ
Okun1.1Z. <- array(aperm(OkunZ, c(2:1, 3)), 
                        dim=c(1, 3, 2*N) )
dim(Okun1.1Z.)
dimnames(Okun1.1Z.) <- list(NULL, OkunStates, Year1.) 
Okun1.1Z.[,,1:4]
Okun1.1Z <- Okun1.1Z.[,,!OkunDel, drop=FALSE]
dim(Okun1.1Z)
(N1.1 <- dim(Okun1.1Z)[3])
Okun1.1Z[,,c(1:2, 9:13)]
``` 

Looks correct.  

The state transition matrix must also vary with time:  It should be OkunT for the first component of the two observations for a given year [log(realGDPperCapita)] and diag(3) for the second [qnorm(unemployment)]:  

```{r}
Okun1.1T. <- array(OkunT, dim=c(3, 3, 2*N), 
  dimnames=list(OkunStates, OkunStates, 
                rep(GDP$Year, each=2)))
Okun1.1T.[,,sameYr] <- diag(3)
Okun1.1T <- Okun1.1T.[,,!OkunDel]
Okun1.1T[,, c(1:2, 9:13)]
```

That looks correct.  Now let's create Okun1.1Q:  

```{r}
Okun1.1Q <- array(diag(NA, 3), dim=c(3,3,N1.1), 
  dimnames=list(OkunStates, OkunStates, Year1))
Okun1.1Q[,,sameYr] <- 0
Okun1.1Q[,,9:14]
```

We can reuse Okun.a1, P1=diag(0, 3) and P1inf=diag(3) in SSMcustom and formula:  

```{r}
Okun1.1Fmla <- formula(OkunDat1 ~ -1 + SSMcustom(
  Z=Okun1.1Z, T=Okun1.1T, Q=Okun1.1Q, a1=Okun.a1, 
  P1=OkunP1, P1inf=OkunP1inf, index=1, n=N1.1,
  state_names=OkunStates) )
```

H is now univariate and must be managed in the update function:  

```{r}
Okun1.1H <- array(NA, dim=c(1,1,N1.1), 
    dimnames=list(NULL, NULL, Year1) )
Okun1.1H[,,9:13]
```

Looks correct.  

We're now ready to call SSModel:  

```{r}
Okun1.1Mdl. <- SSModel(Okun1.1Fmla, H=Okun1.1H)
Okun1.1Mdl <- fixModelNames(Okun1.1Mdl., OkunStates)
```

The parameters to be estimated are the same as for Okun1.  We'll used the same starting values.  However, we need a different updatefn.  First let's create Okun1.1Updt0 with zero correlation between the migration varariances for GDP.lvl and unemp.lvl:  

```{r}
Okun1.1Updt0 <- function(pars=Okun1Init, 
                    model=Okun1.1Mdl, ...){
#  
  Q <- model$Q
  Q[,,!sameYr] <- diag(exp(pars[1:3]))
  model$Q[] <- Q
#
  Yrs <- time(model$y)
  H <- model$H
  H[,,] <- exp(pars[4])
  Hu <- as.numeric(exp(pars[5])+
      ((Year1<1890)*exp(pars[6])) )
  H[1,,sameYr] <- Hu[sameYr]
  model$H[] <- H 
  model
}
str(tst <- Okun1.1Updt0())

Okun1.1Fit0 <- fitSSM(Okun1.1Mdl, Okun1Init, 
                     Okun1.1Updt0)
```

Before we proceed, we must modify logLik2 so it computes correctely rmse.lnUSD and rmse.pbtUnemp:  

```{r}
logLik2.1 <- function(object, ...){
  ll2 <- rep(NA, 5)
  names(ll2) <- c('pars', 'diffuse', 'marginal', 
        'rmse.lnUSD', 'rmse.pbtUnemp')
  ll2 <- data.frame(pars=NA, diffuse=NA, 
      marginal=NA, rmse.lnUSD=NA, rmse.pbtUnemp=NA)
  ll2[1] <- length(object$optim.out$par)
  ll2[2] <- logLik(object$model)
  ll2[3] <- logLik(object$model, marginal=TRUE)
  K. <- KFS(object$model)
  resids <- OkunVec2mat(K.$v)
  Ks <- apply(resids^2, 2, function(x)
        sqrt(mean(x, na.rm=TRUE)))
  ll2[4:5] <- Ks
#
  ll2
}
```

The marginal log(likelihood) here should be at least as large as the sum of the log(likelihoods) from EWMA2 and uEWMA1a:  

```{r}
with(logLiks, sum(marginal[model %in% 
              c('EWMA2', 'uEWMA1.1b')]))
logLik2.1(Okun1.1Fit0)
```

I'm confused:  The logLik for the current model is 361.75 vs. 396.51 for the combination of the two.  Why isn't the marginal log likelihood for the current model at least equal to that of the previous model?  

Is it because the algorithm cannot handle lack of information about unemp.lvl prior to 1800?  

```{r}
Okun1.1KFS0 <- KFS(Okun1.1Fit0$model)
Okun1.1KFS0$d
```

Ignoring that, let's fix Okun1.1Updt to correlation -1 between increments for GDP.lvl and unemp.lvl:  

```{r}
Okun1.1Updt <- function(pars=Okun1Init, 
                    model=Okun1.1Mdl, ...){
#  
  corQ <- diag(3)
  corQ[1, 2] <- (-1)
  corQ[2, 1] <- (-1)
  sQ <- exp(pars[1:3]/2)
  Q1 <- (corQ * outer(sQ, sQ))
#    
  Q <- model$Q
  Q[,,!sameYr] <- Q1
  model$Q[] <- Q
#
  Yrs <- time(model$y)
  H <- model$H
  H[,,] <- exp(pars[4])
  Hu <- as.numeric(exp(pars[5])+
      ((Year1<1890)*exp(pars[6])) )
  H[1,,sameYr] <- Hu[sameYr]
  model$H[] <- H 
  model
}
str(tst <- Okun1.1Updt())

Okun1.1Fit <- fitSSM(Okun1.1Mdl, Okun1Init, 
                     Okun1.1Updt)
logLiks[18, -1] <- logLik2.1(Okun1.1Fit)  
logLiks[1:18, ]
```  

I'm still confused.  

I want to check Q to make sure it has the correlation structure I specified:  

```{r}
Okun1.1Fit.Q <- Okun1.1Fit$model$Q
cov2cor(Okun1.1Fit.Q[,,1])
```

That's correct.  

MATERIAL I WROTE BEFORE I TRIED UNIVARIATE:  

# 19.  Okun's law with a president effect 

As with EWMA2pres3 and uEWMA1pres3, we need to add a "GDP.pres3" component to the state vector of "OkunMdl", making its state transition and migration variance matrices time-varying.  This will also add one additional parameter to be estimated:  "lnQ.pres3".  

For this we will assign the following names to the components of this 4-dimensional state vector:  GDP.lvl, unemp.lvl, GDP.slope, and GDP.pres3.  

For this state vector we will use a state transition matrix of the following form:  

$$T_t = \left[\begin{array}
{rrrr}
1 & 0 & 1 & 1 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & \gamma_t  
\end{array}\right],
$$ 

where $\gamma_t$ = 0 for the last year of a presidency, and 1 otherwise, as before.  

The migration variance $Q_t$ will be block diagonal with the leading 3 x 3 block being migration variance matrix from the simple OkunMdl of the previous section and the [4, 4] element being $q_{\mathrm{p.pres3}} (1-\gamma_t)$.  

Similarly the matrix $P_1$ is zero except for $P_{1,4,4}$, which is $q_{\mathrm{p.pres3}}$.  

Modeling will follow essentially the same steps as in the previous section, except that T and Q were constant before and will be time-varying for this model.  

<center> 
Response matrix -> Z, T, Q, a1, P1, P1inf -> [SSMcustom](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) in 
[formula](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html) 

-> H -> [SSModel](http://rpackages.ianhowson.com/cran/KFAS/man/SSModel.html) -> [(updatefn)](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) -> [fitSSM](http://rpackages.ianhowson.com/cran/KFAS/man/fitSSM.html) -> [KFS](http://rpackages.ianhowson.com/cran/KFAS/man/KFS.html)  
</center>

```{r}
(OkunP3States <- c('GDP.lvl', 'unemp.lvl', 'GDP.slope', 
                   'GDP.pres3'))
(OkunP3Z <- array(c(1,0, 0,1, 0,0, 0,0), dim=c(2, 4, 1), 
    dimnames=list(OkunVars, OkunP3States, NULL) ) )
              
OkunP3T <- array(c(1,0,0,0, 0,1,0,0, 1,0,1,0, 1,0,0,1), 
                  dim=c(4, 4, N), 
  dimnames=list(OkunP3States, OkunP3States, GDP$Year) ) 
OkunP3T[4,4,presLastYr] <- 0 
pres1LastYr3  <- (which(presLastYr)[1] + (-1):1)
OkunP3T[,,pres1LastYr3]

OkunP3Q <- array(0, c(4,4,N), 
   dimnames=list(OkunP3States, OkunP3States, GDP$Year))
OkunP3Q[1:3, 1:3,] <- OkunQ
OkunP3Q[4,4,presLastYr] <- NA 
OkunP3Q[,,pres1LastYr3]

OkunP3.a1 <- c(apply(OkunDat, 2, function(x)x[!is.na(x)][1]), 
             0, 0) 
names(OkunP3.a1) <- OkunP3States
OkunP3.a1

OkunP3P1 <- diag(0, 4)
dimnames(OkunP3P1) <- list(OkunP3States, OkunP3States)
OkunP3P1inf <- OkunP3P1
OkunP3P1[4,4] <- NA
OkunP3P1
diag(OkunP3P1inf)[1:3] <- 1
OkunP3P1inf

OkunP3Fmla <- formula(OkunDat ~ -1 + SSMcustom(
  Z=OkunP3Z, T=OkunP3T, Q=OkunP3Q, a1=OkunP3.a1, 
  P1=OkunP3P1, P1inf=OkunP3P1inf, index=1:2, n=N,
  state_names=OkunP3States) )
OkunP3Mdl. <- SSModel(OkunP3Fmla, H=OkunH)
OkunP3Mdl <- fixModelNames(OkunP3Mdl., OkunP3States, 
                  OkunVars)
```

The Z, T, Q, a1, P1, and P1inf arrays created here all look plausible.  

OTHER MATERIAL THAT SHOULD BE INCLUDED ABOVE:  

`r Citet(bib2, "Knotek2007")` wrote that estimates of Okun's coefficient based on only 20 quarters tend to be higher during expansion than recession (Chart 4, p. 85).

# 20. Okun's law with a war effect 

| GDP/cap & unemp | OkunWar0:  [Okun's law](https://en.wikipedia.org/wiki/Okun's_law) with a war effect 


# 21.  Okun's law considering both presidents and wars 

| GDP/cap & unemp | OkunP3w0:  [Okun's law](https://en.wikipedia.org/wiki/Okun's_law) modeling both president and war 










# Discussion 

The conclusions from the different models tested match the naive analysis that inspired this work:  

  1.  There has been a substantive difference in the performance of the economy between administrations, with no indication of any changes in the rate of improvement in average annual income apart from this administration effect.  
  1.  There is no evidence that any of the three formulations of a war effect can explain any part of the variability in average annual income (GDP per capita, adjusted for inflation) in U.S. economic history.  
  1.  The near-zero estimate for the observation error variance, H, and the migration variance for the slope in many models suggests that the numbers we have were already smoothed to reduce (artificially) the between-year variability.  
  1.  Some time series models are known to have multiple local maxima of the likelihood function for some data sets.  To check for this, it could help to make contour plots of the log(likehood) surface in a sufficiently broad region of the apparent optimum to identify other and perhaps better local optima;  such alternative optima would presumably be visible in plots of a moderately but not excessively broad region of the optimum identified by the standard algorithm.  Such plots could also display a joint confidence region for any interesting pair of parameters.  (We may also wish to use simuluation to calibrate the confidence regions for cases like many considered here where the assumptions of [Wilks's theorem](https://en.wikipedia.org/wiki/Likelihood-ratio_test#Distribution:_Wilks.27s_theorem) do not hold.) It could be interesting to reestimate some of these models with the migration variance for the slope fixed at, e.g., its upper 90th percentile.  This might reduce the variations between administration, but we'd be surprised if it changed the overall conclusions.  This is left for future work.  

# References
```{r results = "asis", echo = FALSE}
PrintBibliography(bib2)
```
